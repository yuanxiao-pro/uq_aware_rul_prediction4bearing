{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "import torch\n",
    "from joblib import dump, load\n",
    "import torch.utils.data as Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# 参数与配置\n",
    "torch.manual_seed(100)  # 设置随机种子，以使实验结果具有可重复性\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载数据集\n",
    "def dataloader(batch_size, workers=2):\n",
    "    # 训练集\n",
    "    train_set = load('../dataresult/train_set')\n",
    "    train_label = load('../dataresult/train_label')\n",
    "    # 测试集\n",
    "    test_set = load('../dataresult/test_set')\n",
    "    test_label = load('../dataresult/test_label')\n",
    "\n",
    "    # 加载数据\n",
    "    train_loader = Data.DataLoader(dataset=Data.TensorDataset(train_set, train_label),\n",
    "                                   batch_size=batch_size, num_workers=workers, drop_last=True)\n",
    "    test_loader = Data.DataLoader(dataset=Data.TensorDataset(test_set, test_label),\n",
    "                                  batch_size=batch_size, num_workers=workers, drop_last=True)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "batch_size = 16\n",
    "# 加载数据\n",
    "train_loader, test_loader = dataloader(batch_size)\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义裁剪模块\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "#  定义 TCN 卷积+残差 模块\n",
    "from torch.nn.utils import parametrizations\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from bayesian_torch.layers import Conv1dReparameterization, LinearReparameterization\n",
    "\n",
    "class BayesianTemporalBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv1dReparameterization(\n",
    "            in_channels, out_channels, kernel_size,\n",
    "            stride=stride, padding=padding, dilation=dilation,\n",
    "            prior_mean=0, prior_variance=0.1, posterior_mu_init=0, posterior_rho_init=-3\n",
    "        )\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = Conv1dReparameterization(\n",
    "            out_channels, out_channels, kernel_size,\n",
    "            stride=stride, padding=padding, dilation=dilation,\n",
    "            prior_mean=0, prior_variance=0.1, posterior_mu_init=0, posterior_rho_init=-3\n",
    "        )\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.downsample = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else None\n",
    "        self.final_relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        kl = 0.0\n",
    "        out, kl1 = self.conv1(x)\n",
    "        out = self.chomp1(out)\n",
    "        kl += kl1\n",
    "        out = self.relu1(out)\n",
    "        out = self.dropout1(out)\n",
    "        out, kl2 = self.conv2(out)\n",
    "        out = self.chomp2(out)\n",
    "        kl += kl2\n",
    "        out = self.relu2(out)\n",
    "        out = self.dropout2(out)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.final_relu(out + res), kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianTCN(nn.Module):\n",
    "    def __init__(self, input_dim, num_channels, kernel_size=2, dropout=0.2, output_dim=1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        self.kl_modules = []\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = input_dim if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            block = BayesianTemporalBlock(\n",
    "                in_channels, out_channels, kernel_size,\n",
    "                stride=1, dilation=dilation_size,\n",
    "                padding=(kernel_size-1)*dilation_size,\n",
    "                dropout=dropout\n",
    "            )\n",
    "            layers.append(block)\n",
    "        self.network = nn.ModuleList(layers)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        # self.fc = LinearReparameterization(\n",
    "        #     in_features=num_channels[-1], out_features=output_dim,\n",
    "        #     prior_mean=0, prior_variance=0.1, posterior_mu_init=0, posterior_rho_init=-3\n",
    "        # )\n",
    "        self.mu = LinearReparameterization(\n",
    "            in_features=num_channels[-1], out_features=output_dim,\n",
    "            prior_mean=0, prior_variance=0.1, posterior_mu_init=0, posterior_rho_init=-3\n",
    "        )\n",
    "        # sigma是对数方差\n",
    "        self.sigma = LinearReparameterization(\n",
    "            in_features=num_channels[-1], out_features=output_dim,\n",
    "            prior_mean=0, prior_variance=0.1, posterior_mu_init=0, posterior_rho_init=-3\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, input_dim] -> [batch, input_dim, seq_len]\n",
    "        x = x.permute(0, 2, 1)\n",
    "        kl = 0.0\n",
    "        for block in self.network:\n",
    "            x, kl_block = block(x)\n",
    "            kl += kl_block\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # out, kl_fc = self.fc(x)\n",
    "        mu, kl_mu = self.mu(x)\n",
    "        sigma, kl_sigma = self.sigma(x)\n",
    "\n",
    "        kl += (kl_mu+kl_sigma)\n",
    "        return mu, sigma, kl\n",
    "\n",
    "    def kl_loss(self):\n",
    "        kl = 0.0\n",
    "        for m in self.children():  # 只遍历直接子模块\n",
    "            if hasattr(m, \"kl_loss\"):\n",
    "                kl += m.kl_loss()\n",
    "        return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1344\n",
      "  1344\n",
      "    32\n",
      "    32\n",
      "  3072\n",
      "  3072\n",
      "    32\n",
      "    32\n",
      "   448\n",
      "    32\n",
      "  6144\n",
      "  6144\n",
      "    64\n",
      "    64\n",
      " 12288\n",
      " 12288\n",
      "    64\n",
      "    64\n",
      "  2048\n",
      "    64\n",
      "    64\n",
      "    64\n",
      "     1\n",
      "     1\n",
      "    64\n",
      "    64\n",
      "     1\n",
      "     1\n",
      "______\n",
      " 48932\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from loss_function import compute_au_nll\n",
    "# 输入数据维度为[batch_size, sequence_length, input_size]\n",
    "# 输出维度为[batch_size, output_dim]\n",
    "input_dim = 14         # 输入特征维度\n",
    "output_dim = 1         # 输出特征维度\n",
    "num_channels = [32, 64]  # 每个TemporalBlock的输出通道数\n",
    "kernel_size = 3        # 卷积核大小\n",
    "dropout = 0.5          # Dropout概率\n",
    "kl_weight = 0.001\n",
    "model = BayesianTCN(\n",
    "    input_dim=input_dim,\n",
    "    num_channels=[32, 64],  # 这里必须是列表\n",
    "    kernel_size=kernel_size,\n",
    "    dropout=dropout,\n",
    "    output_dim=output_dim\n",
    ")\n",
    "\n",
    "# 定义损失函数和优化函数 \n",
    "# loss_function = nn.MSELoss()  # loss\n",
    "loss_function = compute_au_nll\n",
    "learn_rate = 0.0003\n",
    "optimizer = torch.optim.Adam(model.parameters(), learn_rate)  # 优化器\n",
    "\n",
    "# 看下这个网络结构总共有多少个参数\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BayesianTCN(\n",
      "  (network): ModuleList(\n",
      "    (0): BayesianTemporalBlock(\n",
      "      (conv1): Conv1dReparameterization()\n",
      "      (chomp1): Chomp1d()\n",
      "      (relu1): ReLU()\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (conv2): Conv1dReparameterization()\n",
      "      (chomp2): Chomp1d()\n",
      "      (relu2): ReLU()\n",
      "      (dropout2): Dropout(p=0.5, inplace=False)\n",
      "      (downsample): Conv1d(14, 32, kernel_size=(1,), stride=(1,))\n",
      "      (final_relu): ReLU()\n",
      "    )\n",
      "    (1): BayesianTemporalBlock(\n",
      "      (conv1): Conv1dReparameterization()\n",
      "      (chomp1): Chomp1d()\n",
      "      (relu1): ReLU()\n",
      "      (dropout1): Dropout(p=0.5, inplace=False)\n",
      "      (conv2): Conv1dReparameterization()\n",
      "      (chomp2): Chomp1d()\n",
      "      (relu2): ReLU()\n",
      "      (dropout2): Dropout(p=0.5, inplace=False)\n",
      "      (downsample): Conv1d(32, 64, kernel_size=(1,), stride=(1,))\n",
      "      (final_relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (mu): LinearReparameterization()\n",
      "  (sigma): LinearReparameterization()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 train_MSE-Loss: 0.62332242\n",
      "Epoch:  2 train_MSE-Loss: 0.40798503\n",
      "Epoch:  3 train_MSE-Loss: 0.33776862\n",
      "Epoch:  4 train_MSE-Loss: 0.19790356\n",
      "Epoch:  5 train_MSE-Loss: 0.17269044\n",
      "Epoch:  6 train_MSE-Loss: 0.15064684\n",
      "Epoch:  7 train_MSE-Loss: 0.04216006\n",
      "Epoch:  8 train_MSE-Loss: -0.07038242\n",
      "Epoch:  9 train_MSE-Loss: -0.05683769\n",
      "Epoch: 10 train_MSE-Loss: -0.12251263\n",
      "Epoch: 11 train_MSE-Loss: -0.16178051\n",
      "Epoch: 12 train_MSE-Loss: -0.22419164\n",
      "Epoch: 13 train_MSE-Loss: -0.24110943\n",
      "Epoch: 14 train_MSE-Loss: -0.11570708\n",
      "Epoch: 15 train_MSE-Loss: -0.12913435\n",
      "Epoch: 16 train_MSE-Loss: -0.29790810\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m#  模型训练\u001b[39;00m\n\u001b[1;32m     64\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[0;32m---> 65\u001b[0m \u001b[43mmodel_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 24\u001b[0m, in \u001b[0;36mmodel_train\u001b[0;34m(epochs, model, optimizer, loss_function, train_loader, device)\u001b[0m\n\u001b[1;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     23\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m []    \u001b[38;5;66;03m#保存当前epoch的MSE loss和\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seq, labels \u001b[38;5;129;01min\u001b[39;00m train_loader: \n\u001b[1;32m     25\u001b[0m     seq, labels \u001b[38;5;241m=\u001b[39m seq\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# 每次更新参数前都梯度归零和初始化\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/py39/lib/python3.9/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
      "File \u001b[0;32m/opt/conda/envs/py39/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1491\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data, worker_id)\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1491\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1494\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/py39/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1453\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1449\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1450\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1452\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1453\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1455\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/envs/py39/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1284\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1272\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1273\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1284\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1285\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1286\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1287\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1288\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1289\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/py39/lib/python3.9/multiprocessing/queues.py:122\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rlock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# unserialize the data after having released the lock\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/py39/lib/python3.9/site-packages/torch/multiprocessing/reductions.py:543\u001b[0m, in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    541\u001b[0m fd \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 543\u001b[0m     storage \u001b[38;5;241m=\u001b[39m storage_from_cache(\u001b[38;5;28mcls\u001b[39m, \u001b[43mfd_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfd\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m storage \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m storage\n",
      "File \u001b[0;32m/opt/conda/envs/py39/lib/python3.9/site-packages/torch/multiprocessing/reductions.py:529\u001b[0m, in \u001b[0;36mfd_id\u001b[0;34m(fd)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfd_id\u001b[39m(fd):\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;66;03m# Returns a tuple which uniquely identifies a file descriptor. In Mac OS,\u001b[39;00m\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;66;03m# this doesn't work with shared memory handles, which is why we don't\u001b[39;00m\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;66;03m# support the \"file_descriptor\" sharing method on that platform.\u001b[39;00m\n\u001b[0;32m--> 529\u001b[0m     stat \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (stat\u001b[38;5;241m.\u001b[39mst_ino, stat\u001b[38;5;241m.\u001b[39mst_dev)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rc(\"font\", family='Microsoft YaHei')\n",
    "\n",
    "def model_train(epochs, model, optimizer, loss_function, train_loader, device):\n",
    "    model = model.to(device)\n",
    "\n",
    "    # 最低MSE\n",
    "    minimum_mse = 1000.\n",
    "    # 最佳模型\n",
    "    best_model = model\n",
    "\n",
    "    train_mse = []     # 记录在训练集上每个epoch的 MSE 指标的变化情况   平均值\n",
    "  \n",
    "    # 计算模型运行时间\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "         # 训练\n",
    "        model.train()\n",
    "        train_loss = []    #保存当前epoch的MSE loss和\n",
    "        for seq, labels in train_loader: \n",
    "            seq, labels = seq.to(device), labels.to(device)\n",
    "            # 每次更新参数前都梯度归零和初始化\n",
    "            optimizer.zero_grad()\n",
    "            # 前向传播\n",
    "            mu, sigma, kl = model(seq)  #   torch.Size([16, 10])\n",
    "            kl_loss = model.kl_loss()\n",
    "            # 损失计算\n",
    "            nll_loss = loss_function(labels, mu, sigma)\n",
    "            loss = nll_loss + kl_weight * kl_loss\n",
    "            train_loss.append(loss.item()) # 计算 MSE 损失\n",
    "            # 反向传播和参数更新\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        #     break\n",
    "        # break\n",
    "        # 计算总损失\n",
    "        train_av_mseloss = np.average(train_loss) # 平均\n",
    "        train_mse.append(train_av_mseloss)\n",
    "        print(f'Epoch: {epoch+1:2} train_MSE-Loss: {train_av_mseloss:10.8f}')\n",
    "       \n",
    "        # 如果当前模型的 MSE 低于于之前的最佳准确率，则更新最佳模型\n",
    "        #保存当前最优模型参数\n",
    "        if train_av_mseloss < minimum_mse:\n",
    "            minimum_mse = train_av_mseloss\n",
    "            best_model = model# 更新最佳模型的参数\n",
    "         \n",
    "    # 保存最后的参数\n",
    "    # torch.save(model.state_dict(), 'final_model_tcn.pt')\n",
    "    # 保存最好的参数\n",
    "    torch.save(best_model.state_dict(), 'best_model_tcn.pt')\n",
    "    print(f'\\nDuration: {time.time() - start_time:.0f} seconds')\n",
    "    \n",
    "    # 可视化\n",
    "    plt.plot(range(epochs), train_mse, color = 'b',label = 'train_MSE-loss')\n",
    "    plt.legend()\n",
    "    plt.show()   #显示 lable \n",
    "    print(f'min_MSE: {minimum_mse}')\n",
    "\n",
    "#  模型训练\n",
    "epochs = 500\n",
    "model_train(epochs, model, optimizer, loss_function, train_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第一部分，训练集评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型预测\n",
    "# 模型 测试集 \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载模型的状态字典\n",
    "model.load_state_dict(torch.load('best_model_tcn.pt'))\n",
    "model = model.to(device)\n",
    "\n",
    "# 预测数据\n",
    "origin_data = []\n",
    "pre_data = []\n",
    "with torch.no_grad():\n",
    "        for data, label in train_loader:\n",
    "            # 原始标签\n",
    "            origin_lable = label.tolist()\n",
    "            origin_data += origin_lable\n",
    "            model.eval()  # 将模型设置为评估模式\n",
    "            \n",
    "            # 预测\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            mu, sigma, kl = model(data)  # 对测试集进行预测\n",
    "            train_pred = mu.tolist()\n",
    "            # train_pred = train_pred.tolist()\n",
    "            pre_data += train_pred      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# 反归一化处理\n",
    "# 使用相同的均值和标准差对预测结果进行反归一化处理\n",
    "# 反标准化\n",
    "scaler  = load('../dataresult/scaler')\n",
    "origin_data = scaler.inverse_transform(origin_data)\n",
    "pre_data = scaler.inverse_transform(pre_data)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# 模型分数\n",
    "score = r2_score(origin_data, pre_data)\n",
    "print('训练集上 模型分数-R^2:',score)\n",
    "\n",
    "print('*'*50)\n",
    "# 训练集上的预测误差\n",
    "train_mse = mean_squared_error(origin_data, pre_data)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_mae = mean_absolute_error(origin_data, pre_data)\n",
    "print('训练数据集上的均方误差-MSE: ',train_mse)\n",
    "print('训练数据集上的均方根误差-RMSE: ',train_rmse)\n",
    "print('训练数据集上的平均绝对误差-MAE: ',train_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试集评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型 \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载模型的状态字典\n",
    "model.load_state_dict(torch.load('best_model_tcn.pt'))\n",
    "model = model.to(device)\n",
    "\n",
    "# 预测数据\n",
    "test_origin_data = []\n",
    "test__pre_data = []\n",
    "with torch.no_grad():\n",
    "        for data, label in test_loader:\n",
    "            # 原始标签\n",
    "            origin_lable = label.tolist()\n",
    "            test_origin_data += origin_lable\n",
    "            model.eval()  # 将模型设置为评估模式\n",
    "            \n",
    "            # 预测\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            mu, sigma, kl = model(data)  # 对测试集进行预测\n",
    "            pred = mu.tolist()\n",
    "            test__pre_data += pred        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# 反归一化处理\n",
    "# 使用相同的均值和标准差对预测结果进行反归一化处理\n",
    "# 反标准化\n",
    "scaler  = load('../dataresult/scaler')\n",
    "test_origin_data = scaler.inverse_transform(test_origin_data)\n",
    "test__pre_data = scaler.inverse_transform(test__pre_data)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# 模型分数\n",
    "score = r2_score(test_origin_data, test__pre_data)\n",
    "print('测试集上 模型分数-R^2:',score)\n",
    "\n",
    "print('*'*50)\n",
    "# 训练集上的预测误差\n",
    "test_mse = mean_squared_error(test_origin_data, test__pre_data)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(test_origin_data, test__pre_data)\n",
    "print('测试数据集上的均方误差-MSE: ',test_mse)\n",
    "print('测试数据集上的均方根误差-RMSE: ',test_rmse)\n",
    "print('测试数据集上的平均绝对误差-MAE: ',test_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rc(\"font\", family='Microsoft YaHei')\n",
    "\n",
    "# 可视化结果\n",
    "plt.figure(figsize=(12, 6), dpi=100)\n",
    "plt.plot(test_origin_data, label='真实寿命',color='pink')  # 真实值\n",
    "plt.plot(test__pre_data, label='TCN 预测值',color='c')  # 预测值\n",
    "\n",
    "# plt.plot([-1,170],[2.0*0.7,2.0*0.7],c='black',lw=1,ls='--')  # 临界点直线  可自己调整位置\n",
    "\n",
    "plt.xlabel('运行周期/10s', fontsize=12)\n",
    "plt.ylabel('寿命百分比', fontsize=12)\n",
    "plt.title('Bearing1_3 预测结果', fontsize=16)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 保存数据\n",
    "# 保存数据\n",
    "dump(test_origin_data, '../画图对比/tcn_origin') \n",
    "dump(test__pre_data, '../画图对比/tcn_pre') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
