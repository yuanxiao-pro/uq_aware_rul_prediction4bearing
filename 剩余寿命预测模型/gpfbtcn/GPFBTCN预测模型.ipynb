{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import argparse\n",
    "\n",
    "# æ•°æ®å¯è§†åŒ–æ§åˆ¶å‚æ•°\n",
    "PLOT_FEATURES = True  # è®¾ç½®ä¸ºFalseå¯ä»¥è·³è¿‡ç‰¹å¾ç»˜åˆ¶ï¼ŒåŠ å¿«æ•°æ®åŠ è½½é€Ÿåº¦\n",
    "\n",
    "# å‘½ä»¤è¡Œå‚æ•°è§£æ\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--train_xj', type=str, default=None, help='è®­ç»ƒè½´æ‰¿åˆ—è¡¨, é€—å·åˆ†éš”')\n",
    "parser.add_argument('--test_xj', type=str, default=None, help='æµ‹è¯•è½´æ‰¿åˆ—è¡¨, é€—å·åˆ†éš”')\n",
    "parser.add_argument('--context_xj', type=str, default=None, help='ä¸Šä¸‹æ–‡è½´æ‰¿åˆ—è¡¨, é€—å·åˆ†éš”')\n",
    "parser.add_argument('--validation_xj', type=str, default=None, help='éªŒè¯è½´æ‰¿åˆ—è¡¨, é€—å·åˆ†éš”')\n",
    "parser.add_argument('--plot', action='store_true', help='æ˜¯å¦ç»˜åˆ¶ç‰¹å¾å›¾')\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "# å¦‚æœæœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œåˆ™è¦†ç›–é»˜è®¤è®¾ç½®\n",
    "if hasattr(args, 'plot') and args.plot:\n",
    "    PLOT_FEATURES = args.plot\n",
    "\n",
    "with open('../../config/fbtcn_config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(f\"ğŸ¨ ç‰¹å¾å¯è§†åŒ–: {'å¼€å¯' if PLOT_FEATURES else 'å…³é—­'}\")\n",
    "\n",
    "# è§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œä¼˜å…ˆçº§é«˜äºconfig\n",
    "if args.train_xj is not None:\n",
    "    TRAIN_xj = [x.strip() for x in args.train_xj.split(',') if x.strip()]\n",
    "else:\n",
    "    TRAIN_xj = config['train_bearings']\n",
    "if args.test_xj is not None:\n",
    "    TEST_xj = [x.strip() for x in args.test_xj.split(',') if x.strip()]\n",
    "else:\n",
    "    TEST_xj = config['test_bearings']\n",
    "if args.context_xj is not None:\n",
    "    CONTEXT_xj = [x.strip() for x in args.context_xj.split(',') if x.strip()]\n",
    "else:\n",
    "    CONTEXT_xj = config['context_bearings']\n",
    "if args.validation_xj is not None:\n",
    "    VALIDATION_xj = [x.strip() for x in args.validation_xj.split(',') if x.strip()]\n",
    "else:\n",
    "    VALIDATION_xj = config['validation_bearings']\n",
    "\n",
    "batch_size = config['batch_size']\n",
    "test_batch_size = config['test_batch_size']\n",
    "window_size = config['window_size']\n",
    "input_dim = config['input_dim']\n",
    "output_dim = config['output_dim']\n",
    "num_channels = config['num_channels']\n",
    "kernel_size = config['kernel_size']\n",
    "dropout = config['dropout']\n",
    "epochs = config['epochs']\n",
    "learn_rate = config['learn_rate']\n",
    "init_kl_weight = config['kl_weight']\n",
    "forward_pass = config['forward_pass']\n",
    "seed = config['seed']\n",
    "patience = config['patience']\n",
    "opt = config['opt']\n",
    "output_posterior_rho_init = config['output_posterior_rho_init']\n",
    "conv_posterior_rho_init = config['conv_posterior_rho_init']\n",
    "if patience == \"inf\":\n",
    "    patience = epochs\n",
    "else:\n",
    "    patience = int(patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½æ•°æ®\n",
    "import torch\n",
    "from joblib import dump, load\n",
    "import torch.utils.data as Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "# å‚æ•°ä¸é…ç½®\n",
    "torch.manual_seed(seed)  # è®¾ç½®éšæœºç§å­ï¼Œä»¥ä½¿å®éªŒç»“æœå…·æœ‰å¯é‡å¤æ€§\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"å½“å‰è¿è¡Œè®¾å¤‡: {device}\")\n",
    "\n",
    "FPT_dict_xj = {'Bearing1_1': 76, 'Bearing1_2': 44, 'Bearing1_3': 60, 'Bearing1_4': 0, 'Bearing1_5': 39,\n",
    "            'Bearing2_1': 455, 'Bearing2_2': 48, 'Bearing2_3': 327, 'Bearing2_4': 32, 'Bearing2_5': 141,\n",
    "            'Bearing3_1': 2344, 'Bearing3_2': 0, 'Bearing3_3': 340, 'Bearing3_4': 1418, 'Bearing3_5': 9}\n",
    "\n",
    "def plot_bearing_features(data, bearing_name, is_raw_data=True):\n",
    "    \"\"\"ç»˜åˆ¶è½´æ‰¿çš„11ä¸ªç‰¹å¾\"\"\"\n",
    "    feature_names = [\n",
    "        'Kurtosis', 'Fractal Dimension', 'Peak Factor',\n",
    "        'Energy Ratio', 'Spectral Flatness', 'Mean', \n",
    "        'Variance', 'Skewness', 'Peak Vibration', 'DE', 'FFT Mean'\n",
    "    ]\n",
    "    \n",
    "    # å¦‚æœæ˜¯tensorï¼Œè½¬æ¢ä¸ºnumpy\n",
    "    if torch.is_tensor(data):\n",
    "        data = data.numpy()\n",
    "    \n",
    "    # ç¡®ä¿æ•°æ®å½¢çŠ¶æ­£ç¡®\n",
    "    if data.ndim == 3 and data.shape[1] == 1:  # (samples, 1, features)\n",
    "        data = data.squeeze(1)  # å˜ä¸º (samples, features)\n",
    "    \n",
    "    plt.figure(figsize=(18, 12))\n",
    "    plt.suptitle(f'{bearing_name} - 11ä¸ªç‰¹å¾å¯è§†åŒ–', fontsize=16, y=0.98)\n",
    "    \n",
    "    for i in range(min(11, data.shape[1])):\n",
    "        plt.subplot(4, 3, i+1)\n",
    "        plt.plot(data[:, i], linewidth=1.2, alpha=0.8)\n",
    "        plt.title(f'{feature_names[i]}', fontsize=12, pad=10)\n",
    "        plt.xlabel('æ—¶é—´æ­¥')\n",
    "        plt.ylabel('æ•°å€¼')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯\n",
    "        mean_val = np.mean(data[:, i])\n",
    "        std_val = np.std(data[:, i])\n",
    "        plt.text(0.02, 0.98, f'Î¼={mean_val:.3f}\\nÏƒ={std_val:.3f}', \n",
    "                transform=plt.gca().transAxes, fontsize=8, \n",
    "                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # éšè—å¤šä½™çš„å­å›¾\n",
    "    for i in range(11, 12):\n",
    "        plt.subplot(4, 3, i+1)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.93)\n",
    "    plt.show()\n",
    "    \n",
    "    # æ‰“å°æ•°æ®åŸºæœ¬ä¿¡æ¯\n",
    "    print(f\"\\n=== {bearing_name} æ•°æ®ä¿¡æ¯ ===\")\n",
    "    print(f\"æ•°æ®å½¢çŠ¶: {data.shape}\")\n",
    "    print(f\"æ ·æœ¬æ•°é‡: {data.shape[0]}\")\n",
    "    print(f\"ç‰¹å¾æ•°é‡: {data.shape[1]}\")\n",
    "    print(f\"æ•°æ®èŒƒå›´: [{np.min(data):.6f}, {np.max(data):.6f}]\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# åŠ è½½æ•°æ®é›†\n",
    "def dataloader(batch_size, test_batch_size, workers=os.cpu_count(), window_size=7, plot_features=True):\n",
    "    \"\"\"\n",
    "    åŠ è½½æ•°æ®å¹¶å¯é€‰æ‹©æ€§ç»˜åˆ¶ç‰¹å¾\n",
    "    Args:\n",
    "        plot_features: æ˜¯å¦ç»˜åˆ¶æ¯ä¸ªè½´æ‰¿çš„ç‰¹å¾å›¾\n",
    "    \"\"\"\n",
    "    def get_data_and_labels(bearing_list, data_dir='../../datasetresult/xjtu_made', window_size=window_size, is_test=False, plot_features=True):\n",
    "        data_list = []\n",
    "        label_list = []\n",
    "        \n",
    "        for bearing in bearing_list:\n",
    "            # æ–‡ä»¶åæ ¼å¼: c*_Bearing*_*_data, c*_Bearing*_*_label\n",
    "            data_files = [f for f in os.listdir(data_dir) if bearing in f and f.endswith('_data')]\n",
    "            label_files = [f for f in os.listdir(data_dir) if bearing in f and f.endswith('_label')]\n",
    "            \n",
    "            # æŒ‰æ–‡ä»¶åæ’åºï¼Œç¡®ä¿ä¸€ä¸€å¯¹åº”\n",
    "            data_files.sort()\n",
    "            label_files.sort()\n",
    "            \n",
    "            for data_file, label_file in zip(data_files, label_files):\n",
    "                data = load(os.path.join(data_dir, data_file))\n",
    "                label = load(os.path.join(data_dir, label_file))\n",
    "                \n",
    "                # ç»˜åˆ¶ç‰¹å¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰\n",
    "                if plot_features:\n",
    "                    plot_bearing_features(data, bearing, is_raw_data=True)\n",
    "                \n",
    "                # å¦‚æœæ˜¯æµ‹è¯•é›†ï¼Œä¸”FPT_dict_xjä¸­æœ‰è¯¥bearingï¼Œæˆªå–FPTå€¼å¾€åçš„æ•°æ®\n",
    "                # if is_test and bearing in FPT_dict_xj:\n",
    "                #     fpt = FPT_dict_xj[bearing]\n",
    "                #     if fpt > 0:\n",
    "                #         data = data[fpt:]\n",
    "                #         label = label[fpt:]\n",
    "                \n",
    "                data_list.append(data)\n",
    "                label_list.append(label)\n",
    "        \n",
    "        # æ‹¼æ¥æ‰€æœ‰è½´æ‰¿çš„æ•°æ®\n",
    "        if len(data_list) > 0:\n",
    "            data_all = torch.cat([torch.tensor(d, dtype=torch.float32) if not isinstance(d, torch.Tensor) else d for d in data_list], dim=0)\n",
    "            label_all = torch.cat([torch.tensor(l, dtype=torch.float32) if not isinstance(l, torch.Tensor) else l for l in label_list], dim=0)\n",
    "        else:\n",
    "            data_all = torch.empty(0)\n",
    "            label_all = torch.empty(0)\n",
    "        \n",
    "        return data_all, label_all\n",
    "\n",
    "    print(\"=== å¼€å§‹åŠ è½½æ•°æ®å¹¶ç»˜åˆ¶ç‰¹å¾ ===\")\n",
    "    \n",
    "    # åŠ è½½å„ä¸ªæ•°æ®é›†\n",
    "    print(\"\\nğŸ“Š åŠ è½½è®­ç»ƒé›†æ•°æ®...\")\n",
    "    train_set, train_label = get_data_and_labels(TRAIN_xj, plot_features=plot_features)\n",
    "    # è·å–å¹¶æ‰“å°è®­ç»ƒé›†ç¬¬ä¸€ä¸ªæ ·æœ¬\n",
    "    # train_set = train_set[0]\n",
    "    # train_label = train_label[0]\n",
    "    # print(\"\\nğŸ“ è®­ç»ƒé›†ç¬¬ä¸€ä¸ªæ ·æœ¬:\")\n",
    "    # print(f\"å½¢çŠ¶: {train_set.shape}\")\n",
    "    # print(f\"æ•°æ®: {train_set}\")\n",
    "\n",
    "\n",
    "    print(\"\\nğŸ“Š åŠ è½½ä¸Šä¸‹æ–‡æ•°æ®...\")\n",
    "    context_set, context_label = get_data_and_labels(CONTEXT_xj, plot_features=plot_features)\n",
    "    \n",
    "    print(\"\\nğŸ“Š åŠ è½½æµ‹è¯•é›†æ•°æ®...\")\n",
    "    test_set, test_label = get_data_and_labels(TEST_xj, is_test=True, plot_features=plot_features)\n",
    "    \n",
    "    print(\"\\nğŸ“Š åŠ è½½éªŒè¯é›†æ•°æ®...\")\n",
    "    validation_set, validation_label = get_data_and_labels(VALIDATION_xj, plot_features=plot_features)\n",
    "    \n",
    "    print(f\"\\n=== æ•°æ®åŠ è½½å®Œæˆ ===\")\n",
    "    print(f\"è®­ç»ƒé›†å½¢çŠ¶: {train_set.shape}, {train_label.shape}\")\n",
    "    print(f\"æµ‹è¯•é›†å½¢çŠ¶: {test_set.shape}, {test_label.shape}\")\n",
    "    print(f\"ä¸Šä¸‹æ–‡é›†å½¢çŠ¶: {context_set.shape}, {context_label.shape}\")\n",
    "    print(f\"éªŒè¯é›†å½¢çŠ¶: {validation_set.shape}, {validation_label.shape}\")\n",
    "    \n",
    "    # åˆ›å»ºæ•°æ®åŠ è½½å™¨\n",
    "    train_loader = Data.DataLoader(dataset=Data.TensorDataset(train_set, train_label),\n",
    "                                   batch_size=batch_size, num_workers=workers, drop_last=False, shuffle=True)\n",
    "    context_loader = Data.DataLoader(dataset=Data.TensorDataset(context_set, context_label),\n",
    "                                   batch_size=batch_size, num_workers=workers, drop_last=False, shuffle=True)\n",
    "    test_loader = Data.DataLoader(dataset=Data.TensorDataset(test_set, test_label),\n",
    "                                  batch_size=test_batch_size, num_workers=workers, drop_last=False)\n",
    "    validation_loader = Data.DataLoader(dataset=Data.TensorDataset(validation_set, validation_label),\n",
    "                                   batch_size=test_batch_size, num_workers=workers, drop_last=False)\n",
    "    \n",
    "    return train_loader, context_loader, test_loader, validation_loader\n",
    "\n",
    "# åŠ è½½æ•°æ®å¹¶ç»˜åˆ¶ç‰¹å¾\n",
    "train_loader, context_loader, test_loader, validation_loader = dataloader(\n",
    "    batch_size, test_batch_size, window_size=window_size, plot_features=PLOT_FEATURES\n",
    ")\n",
    "\n",
    "print(f\"\\n=== æ•°æ®åŠ è½½å™¨ä¿¡æ¯ ===\")\n",
    "print(f\"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_loader.dataset)}\")\n",
    "print(f\"ä¸Šä¸‹æ–‡é›†æ ·æœ¬æ•°: {len(context_loader.dataset)}\")\n",
    "print(f\"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_loader.dataset)}\")\n",
    "print(f\"éªŒè¯é›†æ ·æœ¬æ•°: {len(validation_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰è£å‰ªæ¨¡å—\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "#  å®šä¹‰ TCN å·ç§¯+æ®‹å·® æ¨¡å—\n",
    "from torch.nn.utils import parametrizations\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from bayesian_torch.layers import Conv1dReparameterization, LinearReparameterization\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from loss_function import compute_au_nll, compute_au_nll_with_pos\n",
    "from function_kl import get_bayesian_model_parameters, get_bayesian_model_mu_rho, calculate_function_kl\n",
    "from metrics import picp, nmpiw, ece, aleatoric_uncertainty, epistemic_uncertainty, ood_detection, sharpness,mae,rmse\n",
    "from stable_fbtcn_training import model_train_stable, StabilizedAUNLL, get_stable_optimizer\n",
    "\n",
    "class BayesianTemporalBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv1dReparameterization(\n",
    "            in_channels, out_channels, kernel_size,\n",
    "            stride=stride, padding=padding, dilation=dilation,\n",
    "            prior_mean=0, prior_variance=1, posterior_mu_init=0, posterior_rho_init=conv_posterior_rho_init\n",
    "        )\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.conv2 = Conv1dReparameterization(\n",
    "            out_channels, out_channels, kernel_size,\n",
    "            stride=stride, padding=padding, dilation=dilation,\n",
    "            prior_mean=0, prior_variance=1, posterior_mu_init=0, posterior_rho_init=conv_posterior_rho_init\n",
    "        )\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.downsample = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else None\n",
    "        # self.downsample = Conv1dReparameterization(\n",
    "        #     in_channels, out_channels, 1,\n",
    "        #     prior_mean=0, prior_variance=1, posterior_mu_init=0, posterior_rho_init=-3\n",
    "        # ) if in_channels != out_channels else None\n",
    "        self.final_relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        kl = 0.0\n",
    "        out, kl1 = self.conv1(x)\n",
    "        out = self.chomp1(out)\n",
    "        kl = kl + kl1\n",
    "        out = self.relu1(out)\n",
    "        out = self.dropout1(out)\n",
    "        out, kl2 = self.conv2(out)\n",
    "        out = self.chomp2(out)\n",
    "        kl = kl + kl2\n",
    "        out = self.relu2(out)\n",
    "        out = self.dropout2(out)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.final_relu(out + res), kl\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰ä¸€ä¸ªçœŸæ­£çš„é«˜æ–¯è¿‡ç¨‹è¾“å‡ºå±‚\n",
    "class GaussianProcessOutputLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    çœŸæ­£çš„é«˜æ–¯è¿‡ç¨‹å›å½’è¾“å‡ºå±‚\n",
    "    ä½¿ç”¨RBFæ ¸å‡½æ•°è¿›è¡Œé«˜æ–¯è¿‡ç¨‹å›å½’\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, num_inducing_points=50, kernel_lengthscale=1.0, kernel_variance=1.0, noise_variance=0.1):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.num_inducing_points = num_inducing_points\n",
    "        \n",
    "        # è¯±å¯¼ç‚¹ (inducing points) - è¿™äº›æ˜¯GPçš„\"æ”¯æŒç‚¹\"\n",
    "        self.inducing_points = nn.Parameter(torch.randn(num_inducing_points, in_features))\n",
    "        \n",
    "        # è¯±å¯¼ç‚¹å¯¹åº”çš„å‡½æ•°å€¼\n",
    "        self.inducing_values = nn.Parameter(torch.randn(num_inducing_points, out_features))\n",
    "        \n",
    "        # æ ¸å‡½æ•°å‚æ•°\n",
    "        self.log_lengthscale = nn.Parameter(torch.log(torch.tensor(kernel_lengthscale)))\n",
    "        self.log_kernel_variance = nn.Parameter(torch.log(torch.tensor(kernel_variance)))\n",
    "        self.log_noise_variance = nn.Parameter(torch.log(torch.tensor(noise_variance)))\n",
    "        \n",
    "        # ç”¨äºæ•°å€¼ç¨³å®šæ€§çš„jitter\n",
    "        self.jitter = 1e-6\n",
    "        \n",
    "    def rbf_kernel(self, x1, x2):\n",
    "        \"\"\"\n",
    "        RBF (Radial Basis Function) æ ¸å‡½æ•°\n",
    "        k(x1, x2) = ÏƒÂ² * exp(-0.5 * ||x1 - x2||Â² / lÂ²)\n",
    "        \"\"\"\n",
    "        lengthscale = torch.exp(self.log_lengthscale)\n",
    "        kernel_variance = torch.exp(self.log_kernel_variance)\n",
    "        \n",
    "        # è®¡ç®—è·ç¦»çŸ©é˜µ\n",
    "        # x1: (n1, d), x2: (n2, d)\n",
    "        dist_sq = torch.cdist(x1, x2, p=2).pow(2)\n",
    "        \n",
    "        # RBFæ ¸\n",
    "        kernel_matrix = kernel_variance * torch.exp(-0.5 * dist_sq / (lengthscale ** 2))\n",
    "        return kernel_matrix\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        é«˜æ–¯è¿‡ç¨‹å‰å‘ä¼ æ’­\n",
    "        x: (batch_size, in_features)\n",
    "        è¿”å›: (mu, sigma) å…¶ä¸­sigmaæ˜¯æ ‡å‡†å·®è€Œä¸æ˜¯æ–¹å·®\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        noise_variance = torch.exp(self.log_noise_variance)\n",
    "        \n",
    "        # è®¡ç®—æ ¸çŸ©é˜µ\n",
    "        K_uu = self.rbf_kernel(self.inducing_points, self.inducing_points)  # (m, m)\n",
    "        K_uf = self.rbf_kernel(self.inducing_points, x)  # (m, n)\n",
    "        K_ff_diag = torch.exp(self.log_kernel_variance).expand(batch_size)  # (n,)å¯¹è§’å…ƒç´ \n",
    "        \n",
    "        # æ·»åŠ å™ªå£°å’Œjitterä»¥ç¡®ä¿æ•°å€¼ç¨³å®šæ€§\n",
    "        K_uu_inv = torch.inverse(K_uu + (noise_variance + self.jitter) * torch.eye(self.num_inducing_points, device=x.device))\n",
    "        \n",
    "        # GPé¢„æµ‹å‡å€¼: Î¼* = K_uf^T K_uu^(-1) f_u\n",
    "        mu = torch.matmul(K_uf.t(), torch.matmul(K_uu_inv, self.inducing_values))  # (n, out_features)\n",
    "        \n",
    "        # GPé¢„æµ‹æ–¹å·®: ÏƒÂ²* = K_ff - K_uf^T K_uu^(-1) K_uf + noise\n",
    "        # è¿™é‡Œæˆ‘ä»¬è®¡ç®—å¯¹è§’å…ƒç´ ï¼ˆæ¯ä¸ªé¢„æµ‹ç‚¹çš„æ–¹å·®ï¼‰\n",
    "        K_uf_Kuu_inv = torch.matmul(K_uu_inv, K_uf)  # (m, n)\n",
    "        var_reduction = torch.sum(K_uf * K_uf_Kuu_inv, dim=0)  # (n,)\n",
    "        variance = K_ff_diag - var_reduction + noise_variance  # (n,)\n",
    "        \n",
    "        # ç¡®ä¿æ–¹å·®ä¸ºæ­£\n",
    "        variance = torch.clamp(variance, min=self.jitter)\n",
    "        sigma = torch.sqrt(variance).unsqueeze(-1).expand(-1, self.out_features)  # (n, out_features)\n",
    "        \n",
    "        return mu, sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "attention_dim = 1  # å› ä¸ºè¾“å…¥çš„æœ€åä¸€ç»´æ˜¯1\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        # å°†è¾“å…¥ç»´åº¦ä»1æ˜ å°„åˆ°attention_dim\n",
    "        self.query = nn.Linear(1, embed_dim)\n",
    "        self.key = nn.Linear(1, embed_dim) \n",
    "        self.value = nn.Linear(1, embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size=1024, seq_len=128, features=1)\n",
    "        # å¯¹æœ€åä¸€ç»´è¿›è¡Œçº¿æ€§å˜æ¢\n",
    "        q = self.query(x)  # (1024, 128, embed_dim)\n",
    "        k = self.key(x)    # (1024, 128, embed_dim)\n",
    "        v = self.value(x)  # (1024, 128, embed_dim)\n",
    "        \n",
    "        # è®¡ç®—æ³¨æ„åŠ›æƒé‡\n",
    "        attention_weights = F.softmax(torch.matmul(q, k.transpose(1, 2)) / torch.sqrt(torch.tensor(self.embed_dim)), dim=-1)\n",
    "        # åº”ç”¨æ³¨æ„åŠ›æƒé‡\n",
    "        output = torch.matmul(attention_weights, v)  # (1024, 128, embed_dim)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianTCN(nn.Module):\n",
    "    def __init__(self, input_dim, num_channels, attention_dim, kernel_size=2, dropout=0.2, output_dim=1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        self.kl_modules = []\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = input_dim if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            block = BayesianTemporalBlock(\n",
    "                in_channels, out_channels, kernel_size,\n",
    "                stride=1, dilation=dilation_size,\n",
    "                padding=(kernel_size-1)*dilation_size,\n",
    "                dropout=dropout\n",
    "            )\n",
    "            layers.append(block)\n",
    "        self.network = nn.ModuleList(layers)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.mu = LinearReparameterization(\n",
    "            in_features=num_channels[-1], out_features=output_dim,\n",
    "            prior_mean=0, prior_variance=1, posterior_mu_init=0, posterior_rho_init=output_posterior_rho_init  # å¢å¤§åˆå§‹åéªŒæ–¹å·®ï¼Œä½¿å‡å€¼é¢„æµ‹æ›´å…·æ³¢åŠ¨æ€§\n",
    "        )\n",
    "        # posterior_rho_init=-5æ—¶ï¼ŒåéªŒçš„å¯¹æ•°æ–¹å·®ä¸º-10 å› ä¸ºå¯¹æ•°æ–¹å·® log(var) = 2 * rhoï¼Œrho=-5ï¼Œæ‰€ä»¥ log(var)=2*(-5)=-10\n",
    "        self.sigma = LinearReparameterization(\n",
    "            in_features=num_channels[-1], out_features=output_dim,\n",
    "            prior_mean=0, prior_variance=1, posterior_mu_init=0, posterior_rho_init=output_posterior_rho_init\n",
    "        )\n",
    "\n",
    "        # å®šä¹‰è‡ªæ³¨æ„åŠ›å±‚\n",
    "        self.attention = SelfAttention(attention_dim)\n",
    "\n",
    "        # è‡ªé€‚åº”å¹³å‡æ± åŒ–\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def generate_init_params(self, sample_input):\n",
    "        \"\"\"\n",
    "        æ ¹æ®è¾“å…¥æ•°æ®çš„å½¢çŠ¶åˆå§‹åŒ–æ¨¡å‹å‚æ•°\n",
    "        Args:\n",
    "            sample_input: è¾“å…¥æ•°æ®çš„ä¸€ä¸ªæ ·æœ¬ï¼ˆç”¨äºæ¨æ–­å‚æ•°å½¢çŠ¶ï¼‰\n",
    "        Returns:\n",
    "            params_dict: åŒ…å«æ‰€æœ‰å‚æ•°çš„å­—å…¸\n",
    "        \"\"\"\n",
    "        # ç¡®ä¿ä¸è®¡ç®—æ¢¯åº¦\n",
    "        with torch.no_grad():\n",
    "            # å‰å‘ä¼ æ’­ä¸€æ¬¡ï¼ˆä¸ä¿å­˜è®¡ç®—å›¾ï¼‰\n",
    "            _ = self.forward(sample_input)\n",
    "            params_dict = {k: v.clone() for k, v in self.state_dict().items()}\n",
    "            return params_dict\n",
    "        \n",
    "    def forward(self, x, feature=False):\n",
    "        # x: [batch, seq_len, input_dim] -> [batch, input_dim, seq_len]\n",
    "        x = x.permute(0, 2, 1)\n",
    "        kl = 0.0\n",
    "        for block in self.network:\n",
    "            x, kl_block = block(x)\n",
    "            kl = kl + kl_block\n",
    "        # x = self.avgpool(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        mu, kl_mu = self.mu(x)\n",
    "        sigma, kl_sigma = self.sigma(x) # sigmaæ˜¯å¯¹æ•°æ–¹å·®\n",
    "        # sigma = torch.log(sigma ** 2) + 1e-8 # æ˜¾å¼åœ°å°†æ–¹å·®å–å¯¹æ•°\n",
    "        sigma = F.softplus(sigma) # sigmaæ˜¯å¯¹æ•°æ–¹å·®, softplusä¿è¯sigma>0\n",
    "        kl = kl + (kl_mu+kl_sigma)\n",
    "        if feature:\n",
    "            return mu, sigma, kl, x\n",
    "        else:\n",
    "            return mu, sigma, kl\n",
    "\n",
    "    def kl_loss(self):\n",
    "        kl = 0.0\n",
    "        for m in self.children():  # åªéå†ç›´æ¥å­æ¨¡å—\n",
    "            if hasattr(m, \"kl_loss\"):\n",
    "                kl = kl + m.kl_loss()\n",
    "        return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "model = BayesianTCN(\n",
    "    input_dim=input_dim,        \n",
    "    num_channels=num_channels,  # è¿™é‡Œå¿…é¡»æ˜¯åˆ—è¡¨\n",
    "    attention_dim=attention_dim,\n",
    "    kernel_size=kernel_size,\n",
    "    dropout=dropout,\n",
    "    output_dim=output_dim\n",
    ")\n",
    "init_model = copy.deepcopy(model)\n",
    "print(model)\n",
    "# å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å‡½æ•° \n",
    "# loss_function = nn.MSELoss()  # loss\n",
    "loss_function = compute_au_nll_with_pos\n",
    "\n",
    "# çœ‹ä¸‹è¿™ä¸ªç½‘ç»œç»“æ„æ€»å…±æœ‰å¤šå°‘ä¸ªå‚æ•°\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è®­ç»ƒè¿‡ç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # è®­ç»ƒæ¨¡å‹ - åŒ…å«éªŒè¯é›†\n",
    "# import time\n",
    "# import torch.nn.functional as F\n",
    "# import matplotlib.pyplot as plt\n",
    "# import itertools\n",
    "\n",
    "# def model_train_with_validation(epochs, model, optimizer, loss_function, train_loader, context_loader, validation_loader, device):\n",
    "#     model = model.to(device)\n",
    "#     # æœ€ä½éªŒè¯æŸå¤±\n",
    "#     minimum_train_loss = float('inf')\n",
    "#     # æœ€ä½³æ¨¡å‹\n",
    "#     best_model_state = None\n",
    "#     train_losses = []     # è®°å½•åœ¨è®­ç»ƒé›†ä¸Šæ¯ä¸ªepochçš„æŸå¤±\n",
    "#     val_losses = []       # è®°å½•åœ¨éªŒè¯é›†ä¸Šæ¯ä¸ªepochçš„æŸå¤±\n",
    "#     # è®¡ç®—æ¨¡å‹è¿è¡Œæ—¶é—´\n",
    "#     start_time = time.time()\n",
    "#     patience_counter = 0\n",
    "#     for epoch in range(epochs):\n",
    "#         # ===================== è®­ç»ƒé˜¶æ®µ =====================\n",
    "#         model.train()\n",
    "#         train_loss = []    # ä¿å­˜å½“å‰epochçš„è®­ç»ƒæŸå¤±\n",
    "#         kl_losses = []     # ä¿å­˜å½“å‰epochçš„KLæŸå¤±\n",
    "#         nll_losses = []    # ä¿å­˜å½“å‰epochçš„NLLæŸå¤±\n",
    "#         # åˆ›å»ºå¾ªç¯çš„context_loaderè¿­ä»£å™¨\n",
    "#         context_iter = itertools.cycle(context_loader)\n",
    "        \n",
    "#         # éå†train_loaderï¼Œå¹¶ä»å¾ªç¯çš„context_iterä¸­è·å–å¯¹åº”çš„contextæ•°æ®\n",
    "#         for i, train_batch in enumerate(train_loader):\n",
    "#             seq, labels = train_batch\n",
    "#             context_batch = next(context_iter)\n",
    "#             context_seq, _ = context_batch\n",
    "#             seq, labels = seq.to(device), labels.to(device)\n",
    "#             context_seq = context_seq.to(device)\n",
    "            \n",
    "#             # æ¯æ¬¡æ›´æ–°å‚æ•°å‰éƒ½æ¢¯åº¦å½’é›¶\n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#             # å‰å‘ä¼ æ’­\n",
    "#             mu, sigma, _ = model(seq)\n",
    "#             params_variational_mean, params_variational_logvar = get_bayesian_model_mu_rho(model)\n",
    "#             kl_loss = calculate_function_kl(params_variational_mean, params_variational_logvar, context_seq, model=model)\n",
    "            \n",
    "#             # æŸå¤±è®¡ç®—\n",
    "#             nll_loss = loss_function(labels, mu, sigma)\n",
    "#             kl_weight = max(init_kl_weight, 1.0 * (1 - epoch/epochs))\n",
    "#             loss = nll_loss + kl_weight * kl_loss\n",
    "            \n",
    "#             # æ£€æŸ¥NaNæˆ–Inf\n",
    "#             if torch.isnan(loss) or torch.isinf(loss):\n",
    "#                 print(f\"Warning: Invalid loss detected at epoch {epoch+1}, batch {i+1}\")\n",
    "#                 continue\n",
    "                \n",
    "#             train_loss.append(loss.item())\n",
    "#             kl_losses.append(kl_loss.item())\n",
    "#             nll_losses.append(nll_loss.item())\n",
    "#             # åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#         # æ›´æ–°å­¦ä¹ ç‡\n",
    "#         scheduler.step()\n",
    "#         # ===================== è®°å½•å’Œæ˜¾ç¤ºæŸå¤± =====================\n",
    "#         if len(train_loss) > 0:\n",
    "#             train_avg_loss = np.average(train_loss)\n",
    "#             train_avg_kl = np.average(kl_losses) if len(kl_losses) > 0 else 0.0\n",
    "#             train_avg_nll = np.average(nll_losses) if len(nll_losses) > 0 else 0.0\n",
    "#             train_losses.append(train_avg_loss)\n",
    "#             # è®­ç»ƒé›†sigma\n",
    "#             if 'sigma' in locals():\n",
    "#                 train_sigma_mean = sigma.mean().item()\n",
    "#             else:\n",
    "#                 train_sigma_mean = float('nan')\n",
    "#             print(f'Epoch: {epoch+1:3} | Train Loss: {train_avg_loss:8.6f} | Train KL: {train_avg_kl:8.2f} | Train NLL: {train_avg_nll:8.2f} | Train Sigma Mean: {train_sigma_mean:.6f}')\n",
    "#         else:\n",
    "#             print(f'Epoch: {epoch+1:3} - No valid batches processed')\n",
    "#             continue\n",
    "       \n",
    "#         # ===================== æ—©åœå’Œæœ€ä½³æ¨¡å‹ä¿å­˜ =====================\n",
    "#         # ä½¿ç”¨éªŒè¯æŸå¤±è¿›è¡Œæ—©åœåˆ¤æ–­\n",
    "#         if train_avg_loss < minimum_train_loss:\n",
    "#             minimum_train_loss = train_avg_loss\n",
    "#             best_model_state = model.state_dict().copy()  # æ·±æ‹·è´æ¨¡å‹çŠ¶æ€\n",
    "#             best_epoch = epoch + 1\n",
    "#             patience_counter = 0\n",
    "#             print(f\"âœ“ New best model at epoch {epoch+1} with train loss: {train_avg_loss:.6f}\")\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "#         if patience_counter >= patience:\n",
    "#             print(f\"Early stopping at epoch {epoch+1}, best epoch was {best_epoch} with train loss: {minimum_train_loss:.6f}\")\n",
    "#             break\n",
    "    \n",
    "#     # ä¿å­˜æœ€å¥½çš„æ¨¡å‹å‚æ•°\n",
    "#     if best_model_state is not None:\n",
    "#         torch.save(best_model_state, 'best_model_fbtcn.pt')\n",
    "#         print(f\"Best model saved with train loss: {minimum_train_loss:.6f}\")\n",
    "    \n",
    "#     print(f'\\nDuration: {time.time() - start_time:.0f} seconds')\n",
    "    \n",
    "#     # ===================== å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹ =====================\n",
    "#     plt.figure(figsize=(12, 5))\n",
    "    \n",
    "#     # å­å›¾1: æŸå¤±æ›²çº¿\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     epochs_range = range(1, len(train_losses) + 1)\n",
    "#     plt.plot(epochs_range, train_losses, 'b-', label='Train Loss', linewidth=2)\n",
    "#     # plt.plot(epochs_range, val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "#     plt.axvline(x=best_epoch, color='g', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch})')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.title('Training Loss')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True, alpha=0.3)\n",
    "    \n",
    "#     # å­å›¾2: æ”¾å¤§åæœŸæŸå¤±å˜åŒ–\n",
    "#     if len(train_losses) > 20:\n",
    "#         plt.subplot(1, 2, 2)\n",
    "#         start_idx = len(train_losses) // 2  # ä»ä¸­é—´å¼€å§‹æ˜¾ç¤º\n",
    "#         plt.plot(epochs_range[start_idx:], train_losses[start_idx:], 'b-', label='Train Loss', linewidth=2)\n",
    "#         # plt.plot(epochs_range[start_idx:], val_losses[start_idx:], 'r-', label='Validation Loss', linewidth=2)\n",
    "#         if best_epoch > start_idx:\n",
    "#             plt.axvline(x=best_epoch, color='g', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch})')\n",
    "#         plt.xlabel('Epoch')\n",
    "#         plt.ylabel('Loss')\n",
    "#         plt.title('Loss (Later Half)')\n",
    "#         plt.legend()\n",
    "#         plt.grid(True, alpha=0.3)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "#     print(f'Minimum train loss: {minimum_train_loss:.6f}')\n",
    "#     return train_losses, val_losses, best_epoch\n",
    "\n",
    "# #  æ¨¡å‹è®­ç»ƒ - ç°åœ¨åŒ…å«éªŒè¯é›†\n",
    "# print(\"å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼ˆåŒ…å«éªŒè¯é›†ï¼‰...\")\n",
    "# train_losses, val_losses, best_epoch = model_train_with_validation(\n",
    "#     epochs, model, optimizer, loss_function, \n",
    "#     train_loader, context_loader, validation_loader, device\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¨³å®šè®­ç»ƒè¿‡ç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = get_stable_optimizer(model, config)\n",
    "# æ›¿æ¢åŸå§‹è®­ç»ƒå‡½æ•° - æ·»åŠ è·³è¿‡éªŒè¯é›†çš„é€‰é¡¹\n",
    "train_losses, val_losses, best_epoch = model_train_stable(\n",
    "    epochs, model, init_model, optimizer,  compute_au_nll,\n",
    "    train_loader, context_loader, validation_loader, device, config,\n",
    "    skip_validation=True,  # è®¾ç½®ä¸ºTrueè·³è¿‡éªŒè¯é›†è®¡ç®—ï¼ŒåŠ å¿«è®­ç»ƒé€Ÿåº¦\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æµ‹è¯•é›†è¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# æ¨¡å‹\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# åŠ è½½æ¨¡å‹çš„çŠ¶æ€å­—å…¸\n",
    "model.load_state_dict(torch.load('best_model_fbtcn_stable.pt'))\n",
    "model = model.to(device)\n",
    "\n",
    "# é¢„æµ‹æ•°æ®\n",
    "test_origin_data = []\n",
    "test_pre_data = []\n",
    "mu_std_list = []  # ä¿®å¤ï¼šç›´æ¥å­˜å‚¨æ¯ä¸ªæ ·æœ¬çš„stdå€¼ï¼Œè€Œä¸æ˜¯æ¯ä¸ªbatchçš„åˆ—è¡¨\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, label in test_loader:\n",
    "        # åŸå§‹æ ‡ç­¾\n",
    "        origin_lable = label.tolist()\n",
    "        test_origin_data += origin_lable\n",
    "        model.eval()  # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ \n",
    "        # æ‰§è¡ŒNæ¬¡å‰å‘ä¼ æ’­ï¼Œå¹¶è®¡ç®—metricsä¸­çš„æŒ‡æ ‡\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        mu_list = []\n",
    "        sigma_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for _ in range(forward_pass):\n",
    "                mu, sigma, kl = model(data)\n",
    "                mu_list.append(mu.cpu().numpy())\n",
    "                sigma_list.append(sigma.cpu().numpy())\n",
    "        mu_samples = np.stack(mu_list, axis=0)  # (N, batch, 1)\n",
    "        sigma_samples = np.stack(sigma_list, axis=0)  # (N, batch, 1)\n",
    "        # è®¡ç®—å‡å€¼å’Œæ–¹å·®\n",
    "        mu_mean = np.mean(mu_samples, axis=0)  # (batch, 1)\n",
    "        mu_std = np.std(mu_samples, axis=0)    # (batch, 1)\n",
    "        # è®°å½•é¢„æµ‹å‡å€¼\n",
    "        test_pre_data += mu_mean.squeeze(-1).tolist()\n",
    "        # ä¿®å¤ï¼šä½¿ç”¨extendè€Œä¸æ˜¯appendï¼Œé¿å…åµŒå¥—åˆ—è¡¨é€ æˆçš„å½¢çŠ¶ä¸ä¸€è‡´é—®é¢˜\n",
    "        mu_std_list.extend(mu_std.squeeze(-1).tolist())\n",
    "        # è®°å½•åŸå§‹æ ‡ç­¾\n",
    "        # test_origin_data += label.cpu().numpy().tolist()  # å·²åœ¨å¤–å±‚æ·»åŠ \n",
    "        # è®¡ç®—metrics\n",
    "        # åªåœ¨æœ€åä¸€æ‰¹æ—¶æ‰“å°ä¸€æ¬¡æŒ‡æ ‡\n",
    "        # if len(test_pre_data) >= len(test_loader.dataset):\n",
    "print(len(test_origin_data))\n",
    "print(len(test_pre_data))\n",
    "print(f\"mu_std_list length: {len(mu_std_list)}\")  # éªŒè¯é•¿åº¦æ˜¯å¦æ­£ç¡®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "scaler_dir = '../../datasetresult/xjtu_made'\n",
    "scaler_filename = f'{TEST_xj[0]}_labeled_scaler'\n",
    "all_files = os.listdir(scaler_dir)\n",
    "matched_files = [f for f in all_files if f.endswith(scaler_filename)]\n",
    "if not matched_files:\n",
    "    raise FileNotFoundError(f\"æœªæ‰¾åˆ°åŒ¹é… {TEST_xj[0]} çš„ scaler æ–‡ä»¶, ç›®å½•: {scaler_dir}\")\n",
    "scaler_path = os.path.join(scaler_dir, matched_files[0])\n",
    "scaler = load(scaler_path)\n",
    "\n",
    "# test_origin_data = scaler.inverse_transform(test_origin_data)\n",
    "# test_pre_data = scaler.inverse_transform(test_pre_data)\n",
    "\n",
    "print(matched_files)\n",
    "print(len(test_origin_data))\n",
    "print(len(test_pre_data))\n",
    "test_origin_data = scaler.inverse_transform(np.array(test_origin_data).reshape(-1, 1)).reshape(-1)\n",
    "test_pre_data = scaler.inverse_transform(np.array(test_pre_data).reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# æ¨¡å‹åˆ†æ•°\n",
    "score = r2_score(test_origin_data, test_pre_data)\n",
    "print('æµ‹è¯•é›†ä¸Š æ¨¡å‹åˆ†æ•°-R^2:',score)\n",
    "\n",
    "print('*'*50)\n",
    "# è®­ç»ƒé›†ä¸Šçš„é¢„æµ‹è¯¯å·®\n",
    "test_mse = mean_squared_error(test_origin_data, test_pre_data)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(test_origin_data, test_pre_data)\n",
    "\n",
    "y_true = np.array(test_origin_data).reshape(-1)\n",
    "y_pred_mean = np.array(test_pre_data).reshape(-1)\n",
    "# è·å–test_pre_dataçš„ä¸Šä¸‹ç•Œï¼ˆç½®ä¿¡åŒºé—´ï¼‰\n",
    "from scipy.stats import norm\n",
    "z = norm.ppf(1 - 0.05 / 2)  # 95%ç½®ä¿¡åŒºé—´\n",
    "# ç”±äºtest_pre_dataå·²åå½’ä¸€åŒ–ï¼Œmu_std_listéœ€ä¹˜ä»¥scaler.scale_å¾—åˆ°åŸç©ºé—´std\n",
    "if hasattr(scaler, 'scale_'):\n",
    "    std_scale = scaler.scale_[0] if scaler.scale_.ndim > 0 else scaler.scale_\n",
    "else:\n",
    "    std_scale = 1.0\n",
    "\n",
    "# ä¿®å¤ï¼šç°åœ¨mu_std_listå·²ç»æ˜¯ä¸€ç»´åˆ—è¡¨ï¼Œç›´æ¥è½¬æ¢ä¸ºnumpyæ•°ç»„å³å¯\n",
    "y_pred_std_origin = np.array(mu_std_list) * std_scale\n",
    "y_pred_mean_origin = np.array(test_pre_data).reshape(-1)\n",
    "test_pre_data_lower = y_pred_mean_origin - z * y_pred_std_origin\n",
    "test_pre_data_upper = y_pred_mean_origin + z * y_pred_std_origin\n",
    "\n",
    "y_pred_std = np.array(mu_std_list)  # ä¿®å¤ï¼šç›´æ¥è½¬æ¢ï¼Œæ— éœ€reshape  \n",
    "\n",
    "print(f\"{'æŒ‡æ ‡åç§°':<25} | {'æ•°å€¼'}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'å‡æ–¹è¯¯å·®-MSE':<25} | {test_mse:.6f}\")\n",
    "print(f\"{'å‡æ–¹æ ¹è¯¯å·®-RMSE':<25} | {test_rmse:.6f}\")\n",
    "print(f\"{'å¹³å‡ç»å¯¹è¯¯å·®-MAE':<25} | {test_mae:.6f}\")\n",
    "print(f\"{'PICP':<25} | {picp(y_true, test_pre_data_lower, test_pre_data_upper):.6f}\")\n",
    "print(f\"{'NMPIW':<25} | {nmpiw(y_true, y_pred_mean, y_pred_std):.6f}\")\n",
    "print(f\"{'ECE':<25} | {ece(y_true, y_pred_mean, y_pred_std):.6f}\")\n",
    "print(f\"{'Aleatoric Uncertainty':<25} | {aleatoric_uncertainty(y_pred_std):.6f}\")\n",
    "print(f\"{'Epistemic Uncertainty':<25} | {epistemic_uncertainty(mu_samples.squeeze(-1)):.6f}\")\n",
    "print(f\"{'Sharpness':<25} | {sharpness(y_pred_std):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# å¯è§†åŒ–ç»“æœ\n",
    "plt.figure(figsize=(12, 6), dpi=100)\n",
    "plt.plot(test_origin_data, label='Label',color='pink')  # çœŸå®å€¼\n",
    "plt.plot(test_pre_data, label='Prediction',color='c')  # é¢„æµ‹å€¼\n",
    "# è®¡ç®—é¢„æµ‹åŒºé—´ï¼ˆå‡è®¾ä½ æœ‰y_pred_meanå’Œy_pred_stdï¼Œä¸”å·²åå½’ä¸€åŒ–ï¼‰\n",
    "# è¿™é‡Œæˆ‘ä»¬å‡è®¾ test_pre_data æ˜¯åå½’ä¸€åŒ–åçš„ y_pred_mean\n",
    "# å¦‚æœæœ‰ y_pred_stdï¼ˆæ ‡å‡†å·®ï¼‰ï¼Œä¹Ÿéœ€è¦åå½’ä¸€åŒ–\n",
    "# è‹¥ mu_std æ˜¯æ ‡å‡†åŒ–ç©ºé—´ä¸‹çš„ stdï¼Œéœ€ä¹˜ä»¥ scaler.scale_ å¾—åˆ°åŸç©ºé—´ std\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦æœ‰ mu_std_listï¼Œå¦‚æœæœ‰åˆ™åå½’ä¸€åŒ–\n",
    "if 'mu_std_list' in locals():\n",
    "    # mu_std: æ ‡å‡†åŒ–ç©ºé—´ä¸‹çš„ std\n",
    "    # scaler.scale_ æ˜¯ shape (1,) æˆ– (n_features,)\n",
    "    if hasattr(scaler, 'scale_'):\n",
    "        std_scale = scaler.scale_[0] if scaler.scale_.ndim > 0 else scaler.scale_\n",
    "    else:\n",
    "        std_scale = 1.0\n",
    "    y_pred_std_origin = np.array(mu_std_list) * std_scale  \n",
    "    y_pred_mean_origin = test_pre_data  # å·²åå½’ä¸€åŒ–\n",
    "    # è®¡ç®—95%ç½®ä¿¡åŒºé—´\n",
    "    from scipy.stats import norm\n",
    "    z = norm.ppf(1 - 0.05 / 2)\n",
    "    lower = y_pred_mean_origin - z * y_pred_std_origin\n",
    "    upper = y_pred_mean_origin + z * y_pred_std_origin\n",
    "    # ç»˜åˆ¶ç½®ä¿¡åŒºé—´\n",
    "    plt.fill_between(np.arange(len(y_pred_mean_origin)), lower, upper, color='c', alpha=0.2, label='95% CI')\n",
    "\n",
    "# plt.plot([-1,170],[2.0*0.7,2.0*0.7],c='black',lw=1,ls='--')  # ä¸´ç•Œç‚¹ç›´çº¿,å¯è‡ªå·±è°ƒæ•´ä½ç½®\n",
    "\n",
    "plt.xlabel('Cycle/10s', fontsize=12)\n",
    "plt.ylabel('RUL', fontsize=12)\n",
    "plt.title(TEST_xj[0] +' Prediction', fontsize=16)\n",
    "plt.legend()\n",
    "plt.savefig(f'../ç”»å›¾å¯¹æ¯”/{TEST_xj[0]}_fbtcn.png')\n",
    "\n",
    "# ä¿å­˜æ•°æ®\n",
    "dump(test_origin_data, '../ç”»å›¾å¯¹æ¯”/fbtcn_origin') \n",
    "dump(test_pre_data, '../ç”»å›¾å¯¹æ¯”/fbtcn_pre') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
