#!/usr/bin/env python3
"""
FBTCNè‡ªåŠ¨è®­ç»ƒè„šæœ¬ï¼ˆæ–°ç‰ˆæœ¬ï¼‰
æ”¯æŒKæŠ˜äº¤å‰éªŒè¯ã€è¶…å‚æ•°æœç´¢å’Œè¯¦ç»†è¿›åº¦æ˜¾ç¤º
"""

import json
import os
import sys
import time
import copy
import random
import re
import logging
from datetime import datetime
import numpy as np
import torch
import torch.nn as nn
import torch.utils.data as Data
import argparse
from joblib import load
from itertools import product
from tqdm import tqdm
from typing import Dict, List, Tuple, Any, Optional
from sklearn.model_selection import KFold
import re
# æ·»åŠ è·¯å¾„ä»¥ä¾¿å¯¼å…¥æ¨¡å—
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from loss_function import compute_au_nll_with_crps_and_pos, compute_au_nll, compute_au_nll_with_crps, compute_au_nll_with_pos
from stable_fbtcn_training import model_train_stable, get_stable_optimizer
# ==================== æ¨¡å‹å®šä¹‰ ====================
from fbtcn_sa_model import BayesianTCN
# ==================== æµ‹è¯•å‡½æ•° ====================
from test_runner import run_test_and_save, save_config, save_model_and_config, evaluate_and_save_metrics
# ==================== ç­‰æ¸—å›å½’æ ¡å‡† ====================
from isotonic_calibration import run_isotonic_calibration
# ==================== æ—¥å¿—è®¾ç½® ====================
_logger_initialized = False

def setup_logger(log_dir: str = None, log_filename: str = None, force_new: bool = False) -> logging.Logger:
    """
    è®¾ç½®æ—¥å¿—è®°å½•å™¨ï¼ŒåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶
    
    Args:
        log_dir: æ—¥å¿—æ–‡ä»¶ç›®å½•ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å½“å‰ç›®å½•ä¸‹çš„logsæ–‡ä»¶å¤¹
        log_filename: æ—¥å¿—æ–‡ä»¶åï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æ—¶é—´æˆ³ç”Ÿæˆ
        force_new: æ˜¯å¦å¼ºåˆ¶åˆ›å»ºæ–°çš„loggerï¼ˆå³ä½¿å·²å­˜åœ¨ï¼‰
    
    Returns:
        logger: é…ç½®å¥½çš„æ—¥å¿—è®°å½•å™¨
    """
    global _logger_initialized
    
    # åˆ›å»ºlogger
    logger = logging.getLogger('FBTCN_Training')
    logger.setLevel(logging.INFO)
    
    # é¿å…é‡å¤æ·»åŠ handlerï¼ˆé™¤éå¼ºåˆ¶æ–°å»ºï¼‰
    if logger.handlers and not force_new:
        return logger
    
    # å¦‚æœå¼ºåˆ¶æ–°å»ºï¼Œæ¸…é™¤ç°æœ‰handlers
    if force_new and logger.handlers:
        for handler in logger.handlers[:]:
            logger.removeHandler(handler)
            handler.close()
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    if log_dir is None:
        log_dir = os.path.join(os.path.dirname(__file__), 'logs')
    os.makedirs(log_dir, exist_ok=True)
    
    # ç”Ÿæˆæ—¥å¿—æ–‡ä»¶å
    if log_filename is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        log_filename = f'fbtcn_training_{timestamp}.log'
    
    log_path = os.path.join(log_dir, log_filename)
    
    # æ–‡ä»¶handler
    file_handler = logging.FileHandler(log_path, encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    file_formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    file_handler.setFormatter(file_formatter)
    
    # æ§åˆ¶å°handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    console_formatter = logging.Formatter('%(message)s')
    console_handler.setFormatter(console_formatter)
    
    # æ·»åŠ handlers
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    if not _logger_initialized or force_new:
        logger.info(f"æ—¥å¿—æ–‡ä»¶å·²åˆ›å»º: {log_path}")
        _logger_initialized = True
    
    return logger

def get_logger() -> logging.Logger:
    """
    è·å–å·²å­˜åœ¨çš„loggerï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºæ–°çš„
    
    Returns:
        logger: æ—¥å¿—è®°å½•å™¨
    """
    logger = logging.getLogger('FBTCN_Training')
    if not logger.handlers:
        return setup_logger()
    return logger

# ==================== å·¥å…·å‡½æ•° ====================
def set_seed(seed_value, deterministic=True, benchmark=False):
    """è®¾ç½®éšæœºç§å­"""
    random.seed(seed_value)
    np.random.seed(seed_value)
    torch.manual_seed(seed_value)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed_value)
        torch.cuda.manual_seed_all(seed_value)
        torch.backends.cudnn.deterministic = deterministic
        torch.backends.cudnn.benchmark = benchmark
    os.environ['PYTHONHASHSEED'] = str(seed_value)


def load_bearing_data(bearing_list: List[str], data_dir: str) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    åŠ è½½è½´æ‰¿æ•°æ®
    
    Args:
        bearing_list: è½´æ‰¿åç§°åˆ—è¡¨
        data_dir: æ•°æ®ç›®å½•
    
    Returns:
        data: æ•°æ®å¼ é‡
        labels: æ ‡ç­¾å¼ é‡
    """
    data_list = []
    label_list = []
    
    for bearing in bearing_list:
        # å°è¯•ä¸¤ç§æ–‡ä»¶æ ¼å¼
        data_files = [f for f in os.listdir(data_dir) if bearing in f and (f.endswith('_fpt_data') or f.endswith('_data'))]
        label_files = [f for f in os.listdir(data_dir) if bearing in f and (f.endswith('_fpt_label') or f.endswith('_label'))]
        
        data_files.sort()
        label_files.sort()
        
        for data_file, label_file in zip(data_files, label_files):
            data = load(os.path.join(data_dir, data_file))
            label = load(os.path.join(data_dir, label_file))
            data_list.append(data)
            label_list.append(label)
    
    if len(data_list) > 0:
        data_all = torch.cat([torch.tensor(d, dtype=torch.float32) if not isinstance(d, torch.Tensor) else d 
                             for d in data_list], dim=0)
        label_all = torch.cat([torch.tensor(l, dtype=torch.float32) if not isinstance(l, torch.Tensor) else l 
                              for l in label_list], dim=0)
    else:
        data_all = torch.empty(0)
        label_all = torch.empty(0)
    
    return data_all, label_all


def create_data_loaders(train_set, train_label, context_set, context_label, 
                       test_set, test_label, validation_set, validation_label,
                       batch_size, test_batch_size, seed, workers=0):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    generator = torch.Generator()
    generator.manual_seed(seed)
    
    train_loader = Data.DataLoader(
        dataset=Data.TensorDataset(train_set, train_label),
        batch_size=batch_size, num_workers=workers, drop_last=False, shuffle=True,
        pin_memory=True, persistent_workers=True if workers > 0 else False,
        generator=generator
    )
    context_loader = Data.DataLoader(
        dataset=Data.TensorDataset(context_set, context_label),
        batch_size=batch_size, num_workers=workers, drop_last=False, shuffle=True,
        pin_memory=True, persistent_workers=True if workers > 0 else False,
        generator=generator
    )
    test_loader = Data.DataLoader(
        dataset=Data.TensorDataset(test_set, test_label),
        batch_size=test_batch_size, num_workers=workers, drop_last=False,
        pin_memory=True, persistent_workers=True if workers > 0 else False
    )
    validation_loader = Data.DataLoader(
        dataset=Data.TensorDataset(validation_set, validation_label),
        batch_size=test_batch_size, num_workers=workers, drop_last=False,
        pin_memory=True, persistent_workers=True if workers > 0 else False
    )
    
    return train_loader, context_loader, test_loader, validation_loader


def evaluate_model_on_validation(model, validation_loader, device, forward_pass=10):
    """
    åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹
    
    Returns:
        validation_loss: éªŒè¯é›†æŸå¤±
    """
    model.eval()
    total_loss = 0.0
    n_samples = 0
    
    with torch.no_grad():
        for data, label in validation_loader:
            data, label = data.to(device), label.to(device)
            mu_list = []
            for _ in range(forward_pass):
                mu, sigma, kl = model(data)
                mu_list.append(mu.cpu().numpy())
            
            mu_mean = np.mean(np.stack(mu_list, axis=0), axis=0)
            loss = np.mean((mu_mean - label.cpu().numpy()) ** 2)
            total_loss += loss * len(label)
            n_samples += len(label)
    
    return total_loss / n_samples if n_samples > 0 else float('inf')


def format_time(seconds: float) -> str:
    """æ ¼å¼åŒ–æ—¶é—´"""
    if seconds < 60:
        return f"{seconds:.1f}ç§’"
    elif seconds < 3600:
        return f"{seconds/60:.1f}åˆ†é’Ÿ"
    else:
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        return f"{hours}å°æ—¶{minutes}åˆ†é’Ÿ"

# ==================== æ•°æ®é›†ä¸åˆ’åˆ†è¾…åŠ©å‡½æ•° ====================

def get_data_dir(dataset_type: str) -> str:
    """æ ¹æ®æ•°æ®é›†ç±»å‹è·å–æ•°æ®ç›®å½•"""
    base_paths = [
        os.path.join(os.path.dirname(__file__), '../../datasetresult'),
        os.path.join(os.path.dirname(__file__), 'datasetresult'),
        'datasetresult'
    ]
    for base_path in base_paths:
        data_dir = os.path.join(base_path, dataset_type)
        if os.path.exists(data_dir):
            return data_dir
    return os.path.join('datasetresult', dataset_type)

def list_bearings_in_dir(data_dir: str) -> List[str]:
    """
    ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ‰«ææ•°æ®ç›®å½•ï¼Œæå–è½´æ‰¿åç§°ï¼ˆå»æ‰ *_fpt_data/_fpt_label/_data/_label è¿™äº›åç¼€ï¼‰
    """
    if not os.path.exists(data_dir):
        return []
    files = os.listdir(data_dir)
    names = set()
    pattern = re.compile(r'^(.+?)(?:_fpt_data|_fpt_label|_data|_label)$')
    for f in files:
        m = pattern.match(f)
        if m:
            names.add(m.group(1))
    # print("list_bearings_in_dir_regex", names)
    return sorted(list(names))

def normalize_name(name: str) -> str:
    """å»é™¤æ•°æ®é›†å‰ç¼€ï¼Œåªä¿ç•™ä» 'Bearing' å¼€å§‹çš„éƒ¨åˆ†ï¼Œç”¨äºé‡å æ£€æŸ¥"""
    if 'Bearing' in name:
        return name[name.index('Bearing'):]
    return name


def build_splits(train_bearings_all: List[str], test_bearings_all: List[str], context_bearings: List[str]) -> List[Dict[str, List[str]]]:
    """
    æ„å»ºé€è½´æ‰¿ 2è®­1æµ‹ åˆ’åˆ†ï¼š
    - æ¯æ¬¡é€‰æ‹©ä¸€ä¸ªæµ‹è¯•è½´æ‰¿
    - è®­ç»ƒé›† = train_bearings_all - context - å½“å‰æµ‹è¯•è½´æ‰¿
    - ç¡®ä¿ä¸‰è€…äº’ä¸é‡å 
    """
    context_norm = {normalize_name(b) for b in context_bearings}
    splits = []
    for test_bearing in test_bearings_all:
        # è·³è¿‡åœ¨ä¸Šä¸‹æ–‡ä¸­çš„æµ‹è¯•è½´æ‰¿
        if normalize_name(test_bearing) in context_norm:
            continue
        test_bearings = [test_bearing]
        train_bearings = [
            b for b in train_bearings_all
            if normalize_name(b) not in context_norm
            and b not in test_bearings
        ]
        if len(train_bearings) == 0:
            continue
        # é‡å æ£€æŸ¥
        assert set(train_bearings).isdisjoint(test_bearings)
        assert all(normalize_name(b) not in context_norm for b in train_bearings)
        splits.append({
            'train_bearings': train_bearings,
            'test_bearings': test_bearings
        })
    return splits


def get_context_bearings_by_type(train_datasets_type: str, train_bearings_all: List[str], context_bearings: List[str]) -> List[str]:
    """
    æ ¹æ®æ•°æ®é›†ç±»å‹ç”Ÿæˆå›ºå®šçš„ context_bearings
    å½“å‰ç­–ç•¥ï¼š
      - å¯¹ xjtu*ï¼šä¼˜å…ˆé€‰æ‹©è®­ç»ƒé›†ä¸­ä»¥ '_1' ç»“å°¾çš„è½´æ‰¿ä½œä¸ºä¸Šä¸‹æ–‡
      - è‹¥æœªæ‰¾åˆ°ï¼Œåˆ™å›é€€åˆ°é…ç½®ä¸­çš„ context_bearings
      - å…¶ä»–ç±»å‹ï¼šç›´æ¥ä½¿ç”¨é…ç½®ä¸­çš„ context_bearings
    """
    if len(context_bearings) == 0:
        return [b for b in train_bearings_all if '_1' in b]
    # print("train_bearings_all", train_bearings_all)
    if train_datasets_type.startswith('xjtu'):
        ctx_opt = [opt.replace('xjtu_', '') for opt in context_bearings if 'xjtu' in opt]
        print("ctx_opt", ctx_opt)
        # ctx = [b for b in train_bearings_all if b.startswith('c') and '_1' in b]
        ctx = [b for opt in ctx_opt for b in train_bearings_all if opt in b]
        print("get_context_bearings_by_type xjtu", ctx)
        if ctx:
            return ctx
    elif train_datasets_type.startswith('femto'):
        # ctx = [b for b in train_bearings_all if b.startswith('Bearing') and '_1' in b]
        ctx_opt = [opt.replace('femto_', '') for opt in context_bearings if 'femto' in opt]
        print("ctx_opt", ctx_opt)
        # ctx = [b for b in train_bearings_all if 'Bearing1_1' in b or 'Bearing2_3' in b or 'Bearing3_1' in b]
        ctx = [b for opt in ctx_opt for b in train_bearings_all if opt in b]
        print("get_context_bearings_by_type femto", ctx)
        if ctx:
            return ctx
    return [b for b in train_bearings_all if '_1' in b]


def build_condition_splits(
    train_bearings_all: List[str],
    test_bearings_all: List[str],
    context_bearings: List[str],
    validation_bearings: List[str] = None,
    same_dataset: bool = True,
    exclude_validation_from_training: bool = True,
) -> List[Dict[str, List[str]]]:
    """
    æŒ‰å·¥å†µåˆ†ç»„çš„ 2è®­1æµ‹ï¼š
    - "å·¥å†µ"æŒ‰ Bearing å‰é¢çš„æ•°å­—æ¥åˆ’åˆ†ï¼ˆä¾‹å¦‚ Bearing3_1, Bearing3_3, Bearing3_5 éƒ½å±äºå·¥å†µ3ï¼‰
    - æ¯æ¬¡é€‰æ‹©ä¸€ä¸ªå·¥å†µä½œä¸ºæµ‹è¯•ï¼Œæµ‹è¯•é›†ä¸ºè¯¥å·¥å†µä¸‹çš„æ‰€æœ‰è½´æ‰¿ï¼ˆæ’é™¤ä¸Šä¸‹æ–‡ï¼Œæ ¹æ®é…ç½®å†³å®šæ˜¯å¦æ’é™¤éªŒè¯é›†ï¼‰
    - å½“ train_datasets_type == test_datasets_typeï¼ˆsame_dataset=Trueï¼‰æ—¶ï¼š
        * è®­ç»ƒé›†ä¸ºå…¶ä½™å·¥å†µçš„è½´æ‰¿ï¼Œæ’é™¤ä¸Šä¸‹æ–‡ï¼Œæ ¹æ®é…ç½®å†³å®šæ˜¯å¦æ’é™¤éªŒè¯é›†ï¼ˆç»å…¸ 2 è®­ 1 æµ‹æŒ‰å·¥å†µï¼‰
      å½“ train_datasets_type != test_datasets_typeï¼ˆsame_dataset=Falseï¼‰æ—¶ï¼š
        * è®­ç»ƒé›†å¯ä»¥ä½¿ç”¨å…¨éƒ¨å·¥å†µæ•°æ®ï¼ˆæ’é™¤ä¸Šä¸‹æ–‡ï¼Œæ ¹æ®é…ç½®å†³å®šæ˜¯å¦æ’é™¤éªŒè¯é›†ï¼‰ï¼Œæ— éœ€å†æ’é™¤ä¸æµ‹è¯•å·¥å†µç›¸åŒçš„æ¡ä»¶
    - è¿™æ · femto åŒåŸŸæƒ…å†µä¸‹åº”ä¸º 3 è½®ï¼šå·¥å†µ1 / å·¥å†µ2 / å·¥å†µ3
    - validation_bearings: éªŒè¯é›†è½´æ‰¿åˆ—è¡¨ï¼Œç”¨äºç­‰æ¸—å›å½’æ ¡å‡†
    - exclude_validation_from_training: æ˜¯å¦ä»è®­ç»ƒä¸­æ’é™¤éªŒè¯é›†ï¼ˆé»˜è®¤Trueï¼Œå³éªŒè¯é›†ä¸å‚ä¸è®­ç»ƒï¼‰
    """
    def condition_key(name: str) -> str:
        """
        å°†å„ç§æ–‡ä»¶å/å‰ç¼€è¿˜åŸä¸ºâ€œç‰©ç†è½´æ‰¿åâ€çš„å·¥å†µé”®ï¼Œä¾‹å¦‚ï¼š
        - c1_Bearing1_1_labeled   -> Bearing1_1
        - c1_Bearing1_1ed         -> Bearing1_1
        - Bearing1_1_labeled      -> Bearing1_1
        - Bearing1_1ed            -> Bearing1_1
        - femto_Bearing2_3_xxx    -> Bearing2_3
        """
        base = normalize_name(name)
        # ä¼˜å…ˆç”¨æ­£åˆ™ç›´æ¥æŠ½å– BearingX_Y
        m = re.search(r'Bearing\d+_\d+', base)
        if m:
            return m.group(0)
        parts = base.split('_')
        return '_'.join(parts[:2]) if len(parts) >= 2 else base

    def condition_group(name: str) -> str:
        """
        å°†ç‰©ç†è½´æ‰¿åè¿›ä¸€æ­¥æ˜ å°„ä¸ºâ€œå·¥å†µç¼–å·â€ï¼Œä¾‹å¦‚ï¼š
        - Bearing1_1 / Bearing1_5 -> '1'
        - Bearing3_2 / Bearing3_5 -> '3'
        è¿™æ ·å°±å¯ä»¥æŒ‰å·¥å†µ 1/2/3 æ¥åˆ’åˆ† 3 è½®è®­ç»ƒã€‚
        """
        key = condition_key(name)  # e.g. Bearing3_5
        # print("condition_group", key)
        m = re.match(r'Bearing(\d+)_\d+', key)
        if m:
            return m.group(1)
        # å…œåº•ï¼šè‹¥ä¸ç¬¦åˆæ¨¡å¼ï¼Œåˆ™ç›´æ¥è¿”å› key
        return key

    context_norm = {normalize_name(b) for b in context_bearings}
    validation_norm = {normalize_name(b) for b in (validation_bearings or [])} if exclude_validation_from_training else set()

    # å…ˆæŒ‰"å·¥å†µç¼–å·"æŠŠæµ‹è¯•é›†è½´æ‰¿åˆ†ç»„ï¼ˆæ¯”å¦‚ femto ä¸‹åº”è¯¥å¾—åˆ° 3 ä¸ªå·¥å†µï¼š'1','2','3'ï¼‰
    group_to_test: Dict[str, List[str]] = {}
    for b in test_bearings_all:
        g = condition_group(b)
        group_to_test.setdefault(g, []).append(b)

    # åŒæ ·æŒ‰å·¥å†µæŠŠè®­ç»ƒé›†è½´æ‰¿åˆ†ç»„
    group_to_train: Dict[str, List[str]] = {}
    for b in train_bearings_all:
        g = condition_group(b)
        group_to_train.setdefault(g, []).append(b)

    splits: List[Dict[str, List[str]]] = []
    for g, test_list_all in group_to_test.items():
        # å½“å‰å·¥å†µ g çš„æµ‹è¯•è½´æ‰¿ï¼ˆæ’é™¤ä¸Šä¸‹æ–‡ï¼Œæ ¹æ®é…ç½®å†³å®šæ˜¯å¦æ’é™¤éªŒè¯é›†ï¼‰
        test_list = [b for b in test_list_all 
                     if normalize_name(b) not in context_norm 
                     and (not exclude_validation_from_training or normalize_name(b) not in validation_norm)]
        if not test_list:
            continue

        # è®­ç»ƒè½´æ‰¿ï¼š
        # - åŒæ•°æ®é›†ç±»å‹ï¼šå…¶ä½™å·¥å†µï¼ˆ!= gï¼‰çš„å…¨éƒ¨è®­ç»ƒè½´æ‰¿ï¼Œæ’é™¤ä¸Šä¸‹æ–‡ï¼Œæ ¹æ®é…ç½®å†³å®šæ˜¯å¦æ’é™¤éªŒè¯é›†
        # - å¼‚æ•°æ®é›†ç±»å‹ï¼šå¯ä»¥ä½¿ç”¨æ‰€æœ‰å·¥å†µçš„è®­ç»ƒè½´æ‰¿ï¼Œæ’é™¤ä¸Šä¸‹æ–‡ï¼Œæ ¹æ®é…ç½®å†³å®šæ˜¯å¦æ’é™¤éªŒè¯é›†ï¼ˆè·¨åŸŸåœºæ™¯ä¸‹ä¸éœ€è¦æ’é™¤ä¸æµ‹è¯•ç›¸åŒå·¥å†µï¼‰
        train_list: List[str] = []
        for other_g, train_bs in group_to_train.items():
            if same_dataset and other_g == g:
                continue
            train_list.extend(
                b for b in train_bs
                if normalize_name(b) not in context_norm
                and (not exclude_validation_from_training or normalize_name(b) not in validation_norm)
            )
        if not train_list:
            continue

        # é‡å ä¸ä¸Šä¸‹æ–‡æ£€æŸ¥
        assert set(train_list).isdisjoint(test_list)
        assert all(normalize_name(b) not in context_norm for b in train_list)
        assert all(normalize_name(b) not in context_norm for b in test_list)
        # åªæœ‰å½“æ’é™¤éªŒè¯é›†æ—¶æ‰æ£€æŸ¥éªŒè¯é›†é‡å 
        if exclude_validation_from_training:
            assert all(normalize_name(b) not in validation_norm for b in train_list)
            assert all(normalize_name(b) not in validation_norm for b in test_list)

        splits.append({
            'condition': g,
            'train_bearings': train_list,
            'test_bearings': test_list,
        })

    return splits

# ==================== è¶…å‚æ•°æœç´¢ ====================

def hyperparameter_search(config: Dict, train_bearings: List[str], context_bearings: List[str],
                         validation_bearings: List[str], device: torch.device, 
                         train_data_dir: str, test_data_dir: str, n_trials: int = 20) -> Dict:
    """
    è¶…å‚æ•°æœç´¢ï¼ˆåœ¨éªŒè¯é›†ä¸Šè¿›è¡Œï¼‰
    
    Args:
        config: é…ç½®å­—å…¸
        train_bearings: è®­ç»ƒé›†è½´æ‰¿åˆ—è¡¨
        context_bearings: ä¸Šä¸‹æ–‡é›†è½´æ‰¿åˆ—è¡¨ï¼ˆå›ºå®šï¼‰
        validation_bearings: éªŒè¯é›†è½´æ‰¿åˆ—è¡¨
        device: è®¾å¤‡
        train_data_dir: è®­ç»ƒæ•°æ®ç›®å½•
        test_data_dir: æµ‹è¯•æ•°æ®ç›®å½•
        n_trials: æœç´¢æ¬¡æ•°
    
    Returns:
        best_params: æœ€ä½³è¶…å‚æ•°
    """
    # print("\n" + "="*80)
    # print("ğŸ” å¼€å§‹è¶…å‚æ•°æœç´¢ï¼ˆåœ¨éªŒè¯é›†ä¸Šè¿›è¡Œï¼‰")
    # print("="*80)
    
    # å®šä¹‰è¶…å‚æ•°æœç´¢ç©ºé—´
    param_grid = {
        'num_channels': [
            [32, 64, 32],
            # [64, 128, 64],
            # [32, 64, 128, 64],
            # [16, 32, 64, 32],
            # [64, 128, 128, 64],
        ],
        'kernel_size': [3, 5, 7, 9],
        # 'dropout': [0.1, 0.2, 0.3, 0.4],
        'learn_rate': [0.001, 0.01, 0.03, 0.05],
        # 'kl_weight': [1e-6, 1e-5, 1e-4, 1e-3],
        # 'output_posterior_rho_init': [-3, -2, -1, 0],
    }
    
    # åŠ è½½æ•°æ®ï¼ˆè®­ç»ƒé›†å’Œä¸Šä¸‹æ–‡é›†ä½¿ç”¨è®­ç»ƒæ•°æ®ç›®å½•ï¼ŒéªŒè¯é›†ä½¿ç”¨æµ‹è¯•æ•°æ®ç›®å½•ï¼‰
    train_set, train_label = load_bearing_data(train_bearings, train_data_dir)
    context_set, context_label = load_bearing_data(context_bearings, train_data_dir)
    validation_set, validation_label = load_bearing_data(validation_bearings, test_data_dir)
    
    if len(train_set) == 0 or len(validation_set) == 0:
        # print("âš ï¸  è­¦å‘Šï¼šè®­ç»ƒé›†æˆ–éªŒè¯é›†ä¸ºç©ºï¼Œè·³è¿‡è¶…å‚æ•°æœç´¢ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
        return {
            'num_channels': config['num_channels'],
            'kernel_size': config['kernel_size'],
            'learn_rate': config['learn_rate'],
        }
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    batch_size = config['batch_size']
    test_batch_size = config['test_batch_size']
    seed = config['seed']
    
    train_loader, context_loader, _, validation_loader = create_data_loaders(
        train_set, train_label, context_set, context_label,
        torch.empty(0), torch.empty(0), validation_set, validation_label,
        batch_size, test_batch_size, seed
    )
    
    # éšæœºæœç´¢
    best_score = float('inf')
    best_params = None
    
    # ç”Ÿæˆéšæœºå‚æ•°ç»„åˆ
    param_combinations = []
    for _ in range(n_trials):
        params = {
            'num_channels': random.choice(param_grid['num_channels']),
            'kernel_size': random.choice(param_grid['kernel_size']),
            'learn_rate': random.choice(param_grid['learn_rate']),
        }
        param_combinations.append(params)
    
    # æœç´¢è¿›åº¦æ¡
    search_pbar = tqdm(param_combinations, desc="è¶…å‚æ•°æœç´¢", ncols=120, 
                       bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]')
    
    for trial_idx, params in enumerate(search_pbar):
        # æ›´æ–°é…ç½®
        trial_config = config.copy()
        trial_config.update(params)
        
        # åˆ›å»ºæ¨¡å‹
        # æ³¨æ„ï¼šdropoutã€kl_weight å’Œ output_posterior_rho_init ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼ï¼Œä¸å‚ä¸æœç´¢
        model = BayesianTCN(
            input_dim=config['input_dim'],
            num_channels=params['num_channels'],
            attention_dim=config.get('attention_dim', 1),
            kernel_size=params['kernel_size'],
            dropout=config['dropout'],  # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼
            output_dim=config['output_dim'],
            output_posterior_rho_init=config['output_posterior_rho_init'],  # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼
            conv_posterior_rho_init=config.get('conv_posterior_rho_init', -2),
            attention_mode=config.get('attention_mode', 'self'),
        ).to(device)
        
        init_model = copy.deepcopy(model)
        optimizer = get_stable_optimizer(model, trial_config)
        
        # å¿«é€Ÿè®­ç»ƒï¼ˆåªè®­ç»ƒå‡ ä¸ªepochè¿›è¡Œè¯„ä¼°ï¼‰
        search_epochs = min(10, config.get('epochs', 1000) // 10)
        
        try:
            train_losses, _, _ = model_train_stable(
                search_epochs, model, init_model, optimizer, compute_au_nll_with_crps_and_pos,
                train_loader, context_loader, validation_loader, device, trial_config,
                skip_validation=True
            )
            
            # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
            val_score = evaluate_model_on_validation(model, validation_loader, device, forward_pass=5)
            
            search_pbar.set_postfix({
                'trial': f"{trial_idx+1}/{n_trials}",
                'val_loss': f"{val_score:.6f}",
                'best': f"{best_score:.6f}" if best_score != float('inf') else "N/A"
            })
            
            if val_score < best_score:
                best_score = val_score
                best_params = params.copy()
                search_pbar.set_description(f"è¶…å‚æ•°æœç´¢ [æœ€ä½³: {best_score:.6f}]")
        
        except Exception as e:
            # print(f"\nâš ï¸  è¯•éªŒ {trial_idx+1} å¤±è´¥: {e}")
            continue
    
    search_pbar.close()
    
    # print(f"\nâœ“ è¶…å‚æ•°æœç´¢å®Œæˆ")
    # print(f"æœ€ä½³éªŒè¯æŸå¤±: {best_score:.6f}")
    # print(f"æœ€ä½³è¶…å‚æ•°:")
    # for key, value in best_params.items():
    #     print(f"  {key}: {value}")
    
    return best_params


# ==================== KæŠ˜äº¤å‰éªŒè¯ ====================

def k_fold_split(bearings: List[str], k: int = 5, seed: int = 42) -> List[Tuple[List[str], List[str]]]:
    """
    KæŠ˜äº¤å‰éªŒè¯åˆ†å‰²
    
    Args:
        bearings: æ‰€æœ‰è½´æ‰¿åˆ—è¡¨
        k: æŠ˜æ•°
        seed: éšæœºç§å­
    
    Returns:
        folds: [(train_bearings, test_bearings), ...] åˆ—è¡¨
    """
    set_seed(seed, deterministic=False, benchmark=False)
    
    bearings_shuffled = bearings.copy()
    random.shuffle(bearings_shuffled)
    
    kf = KFold(n_splits=k, shuffle=True, random_state=seed)
    folds = []
    
    for train_idx, test_idx in kf.split(bearings_shuffled):
        train_bearings = [bearings_shuffled[i] for i in train_idx]
        test_bearings = [bearings_shuffled[i] for i in test_idx]
        folds.append((train_bearings, test_bearings))
    return folds

# ==================== å•æŠ˜è®­ç»ƒ ====================
def train_single_fold(fold_idx: int, train_bearings: List[str], test_bearings: List[str],
                      context_bearings: List[str], config: Dict, device: torch.device,
                      train_data_dir: str, test_data_dir: str, total_folds: int = 1, 
                      total_start_time: float = None) -> Dict:
    """
    è®­ç»ƒå•ä¸ªæŠ˜
    
    Args:
        fold_idx: æŠ˜ç´¢å¼•
        train_bearings: è®­ç»ƒé›†è½´æ‰¿åˆ—è¡¨
        test_bearings: æµ‹è¯•é›†è½´æ‰¿åˆ—è¡¨
        context_bearings: ä¸Šä¸‹æ–‡é›†è½´æ‰¿åˆ—è¡¨
        config: é…ç½®å­—å…¸
        device: è®¾å¤‡
        train_data_dir: è®­ç»ƒæ•°æ®ç›®å½•
        test_data_dir: æµ‹è¯•æ•°æ®ç›®å½•
        total_folds: æ€»æŠ˜æ•°
        total_start_time: æ€»å¼€å§‹æ—¶é—´
    
    Returns:
        results: è®­ç»ƒç»“æœå­—å…¸
    """
    # è·å–loggerï¼ˆå¦‚æœå·²å­˜åœ¨ï¼‰
    logger = get_logger()
    
    # æ˜¾ç¤ºå½“å‰æŠ˜çš„ä¿¡æ¯ï¼ˆæŒ‰ç‰©ç†å·¥å†µç®€æ´å±•ç¤ºï¼‰
    def pretty_names(names: List[str]) -> List[str]:
        keys = set()
        for b in names:
            m = re.search(r'Bearing\d+_\d+', normalize_name(b))
            if m:
                keys.add(m.group(0))
        return sorted(keys)

    logger.info(f"\n{'='*80}")
    logger.info(f"ğŸ“Š æŠ˜ {fold_idx + 1}/{total_folds} - è®­ç»ƒä¿¡æ¯")
    logger.info(f"{'='*80}")
    logger.info(f"è®­ç»ƒé›†è½´æ‰¿(å·¥å†µ): {', '.join(pretty_names(train_bearings))}")
    logger.info(f"æµ‹è¯•é›†è½´æ‰¿(å·¥å†µ): {', '.join(pretty_names(test_bearings))}")
    if context_bearings:
        logger.info(f"ä¸Šä¸‹æ–‡é›†è½´æ‰¿(å·¥å†µ): {', '.join(pretty_names(context_bearings))}")
    
    # åŠ è½½æ•°æ®ï¼ˆè®­ç»ƒé›†å’Œä¸Šä¸‹æ–‡é›†ä½¿ç”¨è®­ç»ƒæ•°æ®ç›®å½•ï¼Œæµ‹è¯•é›†ä½¿ç”¨æµ‹è¯•æ•°æ®ç›®å½•ï¼‰
    train_set, train_label = load_bearing_data(train_bearings, train_data_dir)
    context_set, context_label = load_bearing_data(context_bearings, train_data_dir)
    test_set, test_label = load_bearing_data(test_bearings, test_data_dir)
    
    # éªŒè¯ä¸é‡å 
    train_set_bearings = set(train_bearings)
    test_set_bearings = set(test_bearings)
    context_set_bearings = set(context_bearings)
    
    overlap_train_test = train_set_bearings & test_set_bearings
    overlap_train_context = train_set_bearings & context_set_bearings
    overlap_test_context = test_set_bearings & context_set_bearings
    
    if overlap_train_test or overlap_train_context or overlap_test_context:
        logger.warning(f"\nâš ï¸  è­¦å‘Š: å‘ç°æ•°æ®é‡å !")
        if overlap_train_test:
            logger.warning(f"  è®­ç»ƒé›†ä¸æµ‹è¯•é›†é‡å : {overlap_train_test}")
        if overlap_train_context:
            logger.warning(f"  è®­ç»ƒé›†ä¸ä¸Šä¸‹æ–‡é›†é‡å : {overlap_train_context}")
        if overlap_test_context:
            logger.warning(f"  æµ‹è¯•é›†ä¸ä¸Šä¸‹æ–‡é›†é‡å : {overlap_test_context}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader, context_loader, test_loader, validation_loader = create_data_loaders(
        train_set, train_label, context_set, context_label,
        test_set, test_label, test_set, test_label,  # ä½¿ç”¨æµ‹è¯•é›†ä½œä¸ºéªŒè¯é›†
        config['batch_size'], config['test_batch_size'], config['seed']
    )
    
    # åˆ›å»ºæ¨¡å‹
    model = BayesianTCN(
        input_dim=config['input_dim'],        
        num_channels=config['num_channels'],
        attention_dim=config['attention_dim'],
        kernel_size=config['kernel_size'],
        conv_posterior_rho_init=config['conv_posterior_rho_init'],
        output_posterior_rho_init=config['output_posterior_rho_init'],
        dropout=config['dropout'],
        output_dim=config['output_dim'],
        attention_mode=config.get('attention_mode', 'self')
    ).to(device)
    
    init_model = copy.deepcopy(model)
    optimizer = get_stable_optimizer(model, config)
    
    # è·å–loggerï¼ˆå¦‚æœå·²å­˜åœ¨ï¼‰
    logger = get_logger()
    
    # è®¡ç®—é¢„è®¡æ—¶é—´
    if total_start_time is not None and fold_idx > 0:
        elapsed = time.time() - total_start_time
        avg_time_per_fold = elapsed / fold_idx
        remaining_folds = total_folds - fold_idx
        estimated_remaining = avg_time_per_fold * remaining_folds
        logger.info(f"\né¢„è®¡å‰©ä½™æ—¶é—´: {format_time(estimated_remaining)}")
    
    # è®­ç»ƒ
    epochs = config['epochs']
    logger.info(f"æµ‹è¯•é›†è½´æ‰¿: {test_bearings}")
    text = test_bearings[0]
    if config['train_datasets_type'] == config['test_datasets_type']:
        match = re.search(r'(?:c[123]_)?(Bearing\d+)', text)
        if match:
            condiction = match.group(1)  # æå–ç¬¬ä¸€ä¸ªæ•è·ç»„
            logger.info(f"æå–çš„å·¥å†µæ ‡è¯†: {condiction}")
        else:
            condiction = "Unknown"
            logger.warning(f"æœªèƒ½ä» {text} ä¸­æå–å·¥å†µæ ‡è¯†")
    else:
        condiction = "Bearing"
    
    fold_start_time = time.time()
    res_dir = config['results_dir']+config['train_datasets_type'].split('_')[0]+'_to_'+config['test_datasets_type'].split('_')[0]+'/'
    if not os.path.exists(res_dir):
        os.makedirs(res_dir)
    best_pt_model_name = condiction + '_' + config['best_pt_model_base_name']
    where_best_pt_model_name = res_dir + best_pt_model_name
    logger.info(f"æœ€ä½³æ¨¡å‹ä¿å­˜è·¯å¾„: {where_best_pt_model_name}")
    if config['loss_function'] == 'au_nll':
        loss_function = compute_au_nll
    elif config['loss_function'] == 'au_nll_with_pos':
        loss_function = compute_au_nll_with_pos
    elif config['loss_function'] == 'au_nll_with_crps':
        loss_function = compute_au_nll_with_crps
    elif config['loss_function'] == 'au_nll_with_crps_and_pos':
        loss_function = compute_au_nll_with_crps_and_pos
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æŸå¤±å‡½æ•°: {config['loss_function']}")
    train_losses, val_losses, best_epoch = model_train_stable(
        epochs, model, init_model, optimizer, loss_function,
        train_loader, context_loader, validation_loader, device, config,
        where_best_pt_model_name,
        skip_validation=True
    )
    
    training_time = time.time() - fold_start_time
    
    return {
        'fold_idx': fold_idx,
        'train_bearings': train_bearings,
        'test_bearings': test_bearings,
        'context_bearings': context_bearings,
        'train_losses': train_losses,
        'val_losses': val_losses,
        'best_epoch': best_epoch,
        'training_time': training_time,
        'model': init_model,
        'model_path': where_best_pt_model_name,
        'config': config,
        'test_loader': test_loader
    }


# ==================== ä¸»å‡½æ•° ====================

def main(config):
    """ä¸»å‡½æ•°"""
    # åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
    log_dir = config.get('log_dir', None)
    logger = setup_logger(log_dir=log_dir)
       
    # è®¾ç½®éšæœºç§å­
    use_deterministic = config.get('use_deterministic', True)
    use_benchmark = config.get('use_benchmark', False)
    set_seed(config['seed'], deterministic=use_deterministic, benchmark=use_benchmark)
    
    # è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ ¹æ®é…ç½®ä¸­çš„æ•°æ®é›†ç±»å‹é€‰æ‹©æ•°æ®ç›®å½•ï¼ˆä¸å†ä» json æä¾›è½´æ‰¿åˆ—è¡¨ï¼‰
    train_datasets_type = config.get('train_datasets_type', 'xjtu')
    test_datasets_type = config.get('test_datasets_type', 'xjtu')
    train_data_dir = get_data_dir(train_datasets_type)
    test_data_dir = get_data_dir(test_datasets_type)
    # è‡ªåŠ¨æ‰«æè½´æ‰¿åˆ—è¡¨
    train_bearings_all = list_bearings_in_dir(train_data_dir)
    test_bearings_all = list_bearings_in_dir(test_data_dir)
    if len(train_bearings_all) == 0:
        logger.error(f"è®­ç»ƒæ•°æ®ç›®å½• {train_data_dir} æœªæ‰¾åˆ°ä»»ä½•è½´æ‰¿æ–‡ä»¶")
        raise ValueError(f"è®­ç»ƒæ•°æ®ç›®å½• {train_data_dir} æœªæ‰¾åˆ°ä»»ä½•è½´æ‰¿æ–‡ä»¶")
    if len(test_bearings_all) == 0:
        logger.error(f"æµ‹è¯•æ•°æ®ç›®å½• {test_data_dir} æœªæ‰¾åˆ°ä»»ä½•è½´æ‰¿æ–‡ä»¶")
        raise ValueError(f"æµ‹è¯•æ•°æ®ç›®å½• {test_data_dir} æœªæ‰¾åˆ°ä»»ä½•è½´æ‰¿æ–‡ä»¶")
    
    logger.info(f"è®­ç»ƒæ•°æ®é›†ç±»å‹: {train_datasets_type}, æµ‹è¯•æ•°æ®é›†ç±»å‹: {test_datasets_type}")
    logger.info(f"è®­ç»ƒæ•°æ®ç›®å½•: {train_data_dir}, æµ‹è¯•æ•°æ®ç›®å½•: {test_data_dir}")
    logger.info(f"è®­ç»ƒé›†è½´æ‰¿æ•°é‡: {len(train_bearings_all)}, æµ‹è¯•é›†è½´æ‰¿æ•°é‡: {len(test_bearings_all)}")

    # ç”Ÿæˆå›ºå®šçš„ä¸Šä¸‹æ–‡è½´æ‰¿ï¼ˆåŸºäºæ•°æ®é›†ç±»å‹ï¼Œè‹¥æ— åˆ™å›é€€åˆ°é…ç½®ï¼‰
    context_bearings = get_context_bearings_by_type(
        train_datasets_type,
        train_bearings_all,
        config.get('context_bearings', [])
    )
    
    # ç”ŸæˆéªŒè¯é›†è½´æ‰¿ï¼ˆç”¨äºç­‰æ¸—å›å½’æ ¡å‡†ï¼‰
    validation_bearings = get_context_bearings_by_type(
        train_datasets_type,
        train_bearings_all,
        config.get('validation_bearings', [])
    )
    
    # æ˜¯å¦ä»è®­ç»ƒä¸­æ’é™¤éªŒè¯é›†ï¼ˆé»˜è®¤Trueï¼Œå³éªŒè¯é›†ä¸å‚ä¸è®­ç»ƒï¼‰
    exclude_validation_from_training = config.get('exclude_validation_from_training', True)
    
    if validation_bearings:
        if exclude_validation_from_training:
            logger.info(f"éªŒè¯é›†è½´æ‰¿(ç”¨äºæ ¡å‡†ï¼Œä¸å‚ä¸è®­ç»ƒ): {validation_bearings}")
        else:
            logger.info(f"éªŒè¯é›†è½´æ‰¿(ç”¨äºæ ¡å‡†ï¼ŒåŒæ—¶å‚ä¸è®­ç»ƒ): {validation_bearings}")

    # æ„å»ºæŒ‰å·¥å†µçš„åˆ’åˆ†
    # same_dataset=True è¡¨ç¤º train/test æ¥è‡ªåŒä¸€æ•°æ®é›†ï¼›å¦åˆ™ä¸ºè·¨æ•°æ®é›†åœºæ™¯
    same_dataset = (train_datasets_type == test_datasets_type)

    if same_dataset:
        # åŒåŸŸï¼šç»å…¸ 2 è®­ 1 æµ‹ï¼ŒæŒ‰å·¥å†µåˆ’åˆ†ï¼Œå¤šè½®è®­ç»ƒ
        splits = build_condition_splits(
            train_bearings_all,
            test_bearings_all,
            context_bearings,
            validation_bearings=validation_bearings,
            same_dataset=True,
            exclude_validation_from_training=exclude_validation_from_training,
        )
        if len(splits) == 0:
            raise ValueError("æ„å»ºå·¥å†µåˆ’åˆ†å¤±è´¥ï¼šè¯·æ£€æŸ¥è®­ç»ƒ/æµ‹è¯•ç›®å½•ä¸ä¸Šä¸‹æ–‡é…ç½®æ˜¯å¦å¯¼è‡´ç©ºåˆ’åˆ†")
    else:
        # å¼‚åŸŸï¼šåªè®­ç»ƒ 1 è½®ï¼Œç”¨å…¨éƒ¨è®­ç»ƒé›†å·¥å†µï¼ˆæ’é™¤ä¸Šä¸‹æ–‡ï¼Œæ ¹æ®é…ç½®å†³å®šæ˜¯å¦æ’é™¤éªŒè¯é›†ï¼‰ï¼Œ
        # åç»­åœ¨å¦ä¸€ä¸ªæ•°æ®é›†ä¸Šå¯ä»¥æŒ¨ä¸ªæµ‹è¯•
        context_norm = {normalize_name(b) for b in context_bearings}
        validation_norm = {normalize_name(b) for b in validation_bearings} if exclude_validation_from_training else set()
        train_used = [
            b for b in train_bearings_all
            if normalize_name(b) not in context_norm
            and (not exclude_validation_from_training or normalize_name(b) not in validation_norm)
        ]
        if not train_used:
            raise ValueError("è·¨æ•°æ®é›†åœºæ™¯ä¸‹ï¼Œæ’é™¤ä¸Šä¸‹æ–‡å’ŒéªŒè¯é›†åè®­ç»ƒé›†ä¸ºç©ºï¼Œè¯·æ£€æŸ¥é…ç½®")
        splits = [{
            'condition': 'all',
            'train_bearings': train_used,
            # è¿™é‡Œå…ˆæŠŠå…¨éƒ¨æµ‹è¯•è½´æ‰¿ä¼ å…¥ï¼Œç”¨äºåˆ›å»º DataLoader å’Œè®­ç»ƒé˜¶æ®µçš„è¯„ä¼°ï¼›
            # çœŸæ­£"æŒ¨ä¸ªæµ‹è¯•"çš„ç»†ç²’åº¦ç»“æœå¯ä»¥åœ¨è®­ç»ƒå®Œæˆåå†è°ƒç”¨ test_runner å•ç‹¬å®Œæˆã€‚
            'test_bearings': test_bearings_all,
        }]

    logger.info(f"\n{'='*80}")
    logger.info(f"ğŸ“‹ å¼€å§‹è®­ç»ƒ æ€»è½®æ•°: {len(splits)}")
    logger.info(f"{'='*80}")

    all_results = []
    best_hparams = None
    total_start_time = time.time()

    for loop_idx, split in enumerate(splits):
        test_bearings = split['test_bearings']
        train_bearings = split['train_bearings']

        loop_config = config.copy()
        logger.info(f"è®­ç»ƒè½´æ‰¿: {train_bearings}")
        # è®­ç»ƒ
        result = train_single_fold(
            loop_idx, train_bearings, test_bearings, context_bearings,
            loop_config, device, train_data_dir, test_data_dir,
            total_folds=len(splits), total_start_time=total_start_time
        )
        res_dir = config['results_dir']+task[0].split('_')[0]+'_to_'+task[1].split('_')[0]+'/'
        scaler_dir = config['scaler_dir']+task[1]+'/'
        logger.info(f"ç»“æœä¿å­˜ç›®å½•: {res_dir}")
        logger.info(f"Scalerç›®å½•: {scaler_dir}")
        if not os.path.exists(res_dir):
            os.makedirs(res_dir)
        # æµ‹è¯•ä»£ç 
        # é¦–å…ˆè¿›è¡Œç¬¬ä¸€æ¬¡æµ‹è¯•ï¼ˆæœªæ ¡å‡†ï¼‰
        logger.info(f"\n{'='*80}")
        logger.info("ç¬¬ä¸€æ¬¡æµ‹è¯•ï¼ˆæœªæ ¡å‡†ï¼‰")
        logger.info(f"{'='*80}")
        
        first_test_results = {}
        for bearing_name_original in test_bearings:
            # ä¸ºæ¯ä¸ªè½´æ‰¿åˆ›å»ºå•ç‹¬çš„ test_loader
            single_bearing_test_set, single_bearing_test_label = load_bearing_data([bearing_name_original], test_data_dir)
            if len(single_bearing_test_set) == 0:
                logger.warning(f"âš ï¸  è­¦å‘Š: è½´æ‰¿ {bearing_name_original} æ²¡æœ‰æ•°æ®ï¼Œè·³è¿‡æµ‹è¯•")
                continue
            
            # åˆ›å»ºå•ä¸ªè½´æ‰¿çš„ test_loader
            single_bearing_test_loader = Data.DataLoader(
                dataset=Data.TensorDataset(single_bearing_test_set, single_bearing_test_label),
                batch_size=config['test_batch_size'], num_workers=0, drop_last=False,
                pin_memory=True, persistent_workers=False
            )
            
            # å‡†å¤‡ç”¨äº scaler æŸ¥æ‰¾çš„è½´æ‰¿åï¼ˆä¸èƒ½æ”¹åç¼€ï¼Œåº”è¯¥æˆªå–å‰ç¼€ç›´æ¥æ‹¼æ¥ï¼‰
            bearing_name_for_scaler = bearing_name_original
            if test_datasets_type == 'xjtu_made':
                bearing_name_for_scaler = bearing_name_original.replace("_labeled", "_labeled_fpt_scaler")
            elif test_datasets_type == 'femto_made':
                bearing_name_for_scaler = bearing_name_original.replace("_labeled", "_labeled_fpt_scaler")
            logger.info(f"æµ‹è¯•è½´æ‰¿: {bearing_name_original} (scaleræŸ¥æ‰¾å: {bearing_name_for_scaler})")
            model_path = result['model_path']
            model = result['model']
            model.load_state_dict(torch.load(model_path))
            logger.info(f"æµ‹è¯•åŠ è½½çš„æƒé‡æ–‡ä»¶åœ°å€: {model_path}")
            # ä½¿ç”¨å•ä¸ªè½´æ‰¿çš„ test_loader è¿›è¡Œæµ‹è¯•
            target, prediction, origin_prediction, log_var_list, mu_samples = run_test_and_save(
                model, 
                single_bearing_test_loader, 
                config['forward_pass'], 
                bearing_name_for_scaler, 
                res_dir, 
                scaler_dir, 
                device
            )
            logger.info(f"target shape: {target.shape}")
            if test_datasets_type == 'xjtu_made':
                bearing_name = bearing_name_for_scaler.replace("_labeled_fpt_scaler", "")
            elif test_datasets_type == 'femto_made':
                bearing_name = bearing_name_for_scaler.replace("_labeled_fpt_scaler", "")
            else:
                # å…¶ä»–ç±»å‹ï¼ˆå¦‚ xjtu_made_mscrgatã€femto_made_mscrgat ç­‰ï¼‰ç»Ÿä¸€ç”¨ç›¸åŒè§„åˆ™
                bearing_name = bearing_name_for_scaler.replace("_labeled_fpt_scaler", "") if "_labeled_fpt_scaler" in bearing_name_for_scaler else bearing_name_original
            
            # ä¿å­˜ç¬¬ä¸€æ¬¡æµ‹è¯•ç»“æœ
            evaluate_and_save_metrics(target, prediction, origin_prediction, log_var_list, mu_samples, 
                                     res_dir+bearing_name+'.csv', res_dir+bearing_name+'.png', 0.05)
            save_config(config, res_dir+bearing_name+'.json')
            
            # ä¿å­˜ç¬¬ä¸€æ¬¡æµ‹è¯•ç»“æœç”¨äºåç»­æ ¡å‡†
            first_test_results[bearing_name] = {
                'target': target,
                'prediction': prediction,
                'origin_prediction': origin_prediction,
                'log_var_list': log_var_list,
                'mu_samples': mu_samples,
                'test_loader': single_bearing_test_loader
            }
            all_results.append(result)
        
        # ç­‰æ¸—å›å½’æ ¡å‡†
        # ä½¿ç”¨éªŒè¯é›†è¿›è¡Œæ ¡å‡†ï¼ˆæŒ‰ç…§ notebook ä¸­çš„æ–¹æ³•ï¼‰
        # å¦‚æœé…ç½®äº† validation_bearingsï¼Œåˆ™ä½¿ç”¨éªŒè¯é›†ï¼›å¦åˆ™å›é€€åˆ°è®­ç»ƒé›†çš„ç¬¬ä¸€ä¸ªå­é›†
        if validation_bearings and len(validation_bearings) > 0:
            # æ˜¯å¦åªä½¿ç”¨ä¸æµ‹è¯•é›†åŒå·¥å†µçš„éªŒè¯é›†ï¼ˆä»é…ç½®ä¸­è¯»å–ï¼Œé»˜è®¤ä¸ºTrueï¼‰
            use_same_condition_validation = config.get('use_same_condition_validation', True)
            if use_same_condition_validation:
                logger.info(f"ä½¿ç”¨ä¸æµ‹è¯•é›†åŒå·¥å†µçš„éªŒè¯é›†è¿›è¡Œç­‰æ¸—å›å½’æ ¡å‡†")
                logger.info(f"å…¨éƒ¨éªŒè¯é›†: {validation_bearings}")
                logger.info(f"æµ‹è¯•é›†: {test_bearings}")
            else:
                logger.info(f"ä½¿ç”¨å…¨éƒ¨éªŒè¯é›†è¿›è¡Œç­‰æ¸—å›å½’æ ¡å‡†: {validation_bearings}")
            run_isotonic_calibration(
                model=model,
                train_bearings=train_bearings,
                train_data_dir=train_data_dir,
                scaler_dir=scaler_dir,
                test_bearings=test_bearings,
                test_datasets_type=test_datasets_type,
                first_test_results=first_test_results,
                config=config,
                device=device,
                res_dir=res_dir,
                load_bearing_data_func=load_bearing_data,
                calibration_bearings=validation_bearings,  # ä½¿ç”¨éªŒè¯é›†
                calibration_mode=config.get('calibration_mode', 'train_first'),  # ä½¿ç”¨è‡ªå®šä¹‰éªŒè¯é›†
                context_bearings=context_bearings,
                use_same_condition_validation=use_same_condition_validation,  # æ˜¯å¦åªä½¿ç”¨åŒå·¥å†µçš„éªŒè¯é›†
                test_data_dir=test_data_dir  # ä¼ é€’æµ‹è¯•æ•°æ®ç›®å½•
            )
        else:
            logger.warning("âš ï¸  è­¦å‘Š: æœªé…ç½®éªŒè¯é›†ï¼Œä½¿ç”¨è®­ç»ƒé›†çš„ç¬¬ä¸€ä¸ªå­é›†è¿›è¡Œæ ¡å‡†")
            run_isotonic_calibration(
                model=model,
                train_bearings=train_bearings,
                train_data_dir=train_data_dir,
                scaler_dir=scaler_dir,
                test_bearings=test_bearings,
                test_datasets_type=test_datasets_type,
                first_test_results=first_test_results,
                config=config,
                device=device,
                res_dir=res_dir,
                load_bearing_data_func=load_bearing_data,
                calibration_bearings=config.get('calibration_bearings', None),
                calibration_mode=config.get('calibration_mode', 'train_first'),
                context_bearings=context_bearings,
                test_data_dir=test_data_dir  # ä¼ é€’æµ‹è¯•æ•°æ®ç›®å½•
            )

    total_time = time.time() - total_start_time

    logger.info(f"\n{'='*80}")
    logger.info(f"âœ… æ‰€æœ‰æµ‹è¯•å¾ªç¯å®Œæˆ")
    logger.info(f"{'='*80}")
    logger.info(f"æ€»è®­ç»ƒæ—¶é—´: {format_time(total_time)}")
    logger.info(f"å¹³å‡æ¯è½®æ—¶é—´: {format_time(total_time / len(splits))}")
    return {
        'all_results': all_results,
        'best_hparams': best_hparams if len(all_results) > 0 else None,
        'config': config,
        'total_time': total_time,
    }


if __name__ == "__main__":
    # åˆå§‹åŒ–å…¨å±€logger
    logger = setup_logger()
    logger.info("="*80)
    logger.info("è‡ªåŠ¨è®­ç»ƒè„šæœ¬å¯åŠ¨!!!")
    logger.info("="*80)

    # å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser()
    parser.add_argument('--config_path', type=str, default=None, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    args, unknown = parser.parse_known_args()
    config_path = args.config_path
    if config_path is None:
        config_path = os.path.join(os.path.dirname(__file__), '../../config/ablation/A_fbtcn_config_ablation_no_rds_all_data.json')
    if not os.path.exists(config_path):
        raise ValueError(f"é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨")
    logger.info(f"ä½¿ç”¨çš„é…ç½®å‚æ•°jsonä¸ºï¼š{config_path}")

    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
        logger.info(f"é…ç½®å‚æ•°: {config}")

    # tasks = [['femto_made', 'femto_made'], ['xjtu_made', 'xjtu_made'], ['xjtu_made', 'femto_made'], ['femto_made', 'xjtu_made']]
    # tasks = [['xjtu_made', 'xjtu_made']]
    tasks = [['xjtu_made_v3', 'xjtu_made_v3']]

    total_script_start_time = time.time()
    for task_idx, task in enumerate(tasks):
        logger.info(f"\nğŸš€ è®­ç»ƒä»»åŠ¡ {task_idx + 1}/{len(tasks)}: {task[0]} -> {task[1]}")
        config['train_datasets_type'] = task[0]
        config['test_datasets_type'] = task[1]
        results = main(config)
        logger.info(f"\nè®­ç»ƒä»»åŠ¡ {task[0]} -> {task[1]} å®Œæˆï¼ç»“æœå·²è¿”å›ï¼Œå¯ä»¥è‡ªè¡Œä¿å­˜ã€‚")
    
    total_script_end_time = time.time()
    logger.info("="*80)
    logger.info(f"âœ… æ‰€æœ‰è®­ç»ƒä»»åŠ¡å®Œæˆï¼æ€»è®­ç»ƒæ—¶é—´: {format_time(total_script_end_time - total_script_start_time)}")
    logger.info("="*80)
