"""
ç­‰æ¸—å›å½’æ ¡å‡†æ¨¡å—
Isotonic Regression Calibration Module

æä¾›ç­‰æ¸—å›å½’æ ¡å‡†çš„å®Œæ•´åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
1. åœ¨ä¸Šä¸‹æ–‡æ•°æ®é›†ä¸Šè¿›è¡Œé¢„æµ‹
2. æ‹Ÿåˆç­‰æ¸—å›å½’æ ¡å‡†å™¨
3. å¯¹æµ‹è¯•é›†è¿›è¡Œæ ¡å‡†
4. è®¡ç®—å’Œä¿å­˜æ ¡å‡†åçš„ç»“æœ
"""

import os
import sys
import numpy as np
import torch
import torch.utils.data as Data
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
from joblib import load
from typing import Dict, List, Tuple, Optional, Any
import logging
import re

# è®¾ç½®matplotlibæ”¯æŒä¸­æ–‡æ˜¾ç¤ºï¼ˆå®‹ä½“ï¼‰å’Œè¥¿æ–‡ï¼ˆTimes New Romanï¼‰
try:
    # å°è¯•ä»é€šç”¨é…ç½®æ¨¡å—å¯¼å…¥
    sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))
    from matplotlib_chinese_config import setup_chinese_font
    # è®¾ç½®ä¸­æ–‡å­—ä½“ä¸ºå®‹ä½“ï¼Œè¥¿æ–‡å­—ä½“ä¸ºTimes New Roman
    setup_chinese_font(chinese_font_name='SimSun', western_font_name='Times New Roman')
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ°é…ç½®
    def setup_chinese_font():
        """é…ç½®matplotlibä»¥æ”¯æŒä¸­æ–‡æ˜¾ç¤ºï¼ˆå®‹ä½“ï¼‰å’Œè¥¿æ–‡ï¼ˆTimes New Romanï¼‰"""
        available_fonts = [f.name for f in matplotlib.font_manager.fontManager.ttflist]
        
        # ä¸­æ–‡å­—ä½“ï¼šå®‹ä½“
        chinese_font_list = ['SimSun', 'NSimSun', 'STSong', 'Songti SC']
        chinese_font = None
        for font in chinese_font_list:
            if font in available_fonts:
                chinese_font = font
                break
        
        # è¥¿æ–‡å­—ä½“ï¼šTimes New Roman
        western_font_list = ['Times New Roman', 'Times', 'DejaVu Serif']
        western_font = None
        for font in western_font_list:
            if font in available_fonts:
                western_font = font
                break
        
        if chinese_font:
            plt.rcParams['font.sans-serif'] = [chinese_font] + plt.rcParams['font.sans-serif']
        if western_font:
            plt.rcParams['font.serif'] = [western_font] + plt.rcParams['font.serif']
            plt.rcParams['mathtext.fontset'] = 'stix'
        
        plt.rcParams['axes.unicode_minus'] = False
        return chinese_font, western_font
    
    setup_chinese_font()

# æ·»åŠ è·¯å¾„ä»¥ä¾¿å¯¼å…¥æ¨¡å—
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from prediction_interval_calibration import IsotonicRegressionCalibrator, evaluate_calibration
from metrics import mae, rmse, picp, nmpiw, ece, cwc, sharpness, aleatoric_uncertainty, epistemic_uncertainty


def normalize_name(name: str) -> str:
    """å»é™¤æ•°æ®é›†å‰ç¼€ï¼Œåªä¿ç•™ä» 'Bearing' å¼€å§‹çš„éƒ¨åˆ†ï¼Œç”¨äºé‡å æ£€æŸ¥"""
    if 'Bearing' in name:
        return name[name.index('Bearing'):]
    return name


def condition_group(name: str) -> str:
    """
    å°†ç‰©ç†è½´æ‰¿åè¿›ä¸€æ­¥æ˜ å°„ä¸º"å·¥å†µç¼–å·"ï¼Œä¾‹å¦‚ï¼š
    - Bearing1_1 / Bearing1_5 -> '1'
    - Bearing3_2 / Bearing3_5 -> '3'
    è¿™æ ·å°±å¯ä»¥æŒ‰å·¥å†µ 1/2/3 æ¥åˆ’åˆ†ã€‚
    """
    def condition_key(name: str) -> str:
        """å°†å„ç§æ–‡ä»¶å/å‰ç¼€è¿˜åŸä¸º"ç‰©ç†è½´æ‰¿å"çš„å·¥å†µé”®"""
        base = normalize_name(name)
        # ä¼˜å…ˆç”¨æ­£åˆ™ç›´æ¥æŠ½å– BearingX_Y
        m = re.search(r'Bearing\d+_\d+', base)
        if m:
            return m.group(0)
        parts = base.split('_')
        return '_'.join(parts[:2]) if len(parts) >= 2 else base
    
    key = condition_key(name)  # e.g. Bearing3_5
    m = re.match(r'Bearing(\d+)_\d+', key)
    if m:
        return m.group(1)
    # å…œåº•ï¼šè‹¥ä¸ç¬¦åˆæ¨¡å¼ï¼Œåˆ™ç›´æ¥è¿”å› key
    return key


def filter_validation_by_condition(validation_bearings: List[str], test_bearings: List[str]) -> List[str]:
    """
    æ ¹æ®æµ‹è¯•é›†çš„å·¥å†µç­›é€‰éªŒè¯é›†ï¼Œåªè¿”å›ä¸æµ‹è¯•é›†åŒå·¥å†µçš„éªŒè¯é›†è½´æ‰¿
    
    Args:
        validation_bearings: éªŒè¯é›†è½´æ‰¿åˆ—è¡¨
        test_bearings: æµ‹è¯•é›†è½´æ‰¿åˆ—è¡¨
    
    Returns:
        ä¸æµ‹è¯•é›†åŒå·¥å†µçš„éªŒè¯é›†è½´æ‰¿åˆ—è¡¨
    """
    if not validation_bearings or not test_bearings:
        return []
    
    # è·å–æµ‹è¯•é›†çš„å·¥å†µç¼–å·é›†åˆ
    test_conditions = set()
    for test_bearing in test_bearings:
        condition = condition_group(test_bearing)
        test_conditions.add(condition)
    
    # ç­›é€‰å‡ºä¸æµ‹è¯•é›†åŒå·¥å†µçš„éªŒè¯é›†è½´æ‰¿
    filtered_validation = []
    for val_bearing in validation_bearings:
        val_condition = condition_group(val_bearing)
        if val_condition in test_conditions:
            filtered_validation.append(val_bearing)
    
    return filtered_validation


def get_logger():
    """è·å–logger"""
    logger = logging.getLogger('FBTCN_Training')
    if not logger.handlers:
        logger = logging.getLogger(__name__)
    return logger


def predict_on_calibration_dataset(
    model: torch.nn.Module,
    calibration_bearings: List[str],
    train_data_dir: str,
    scaler_dir: str,
    test_bearings: List[str],
    test_datasets_type: str,
    config: Dict,
    device: torch.device,
    load_bearing_data_func,
    test_data_dir: Optional[str] = None,
    use_test_data_dir: bool = False
) -> Tuple[Optional[np.ndarray], Optional[np.ndarray], Optional[np.ndarray]]:
    """
    åœ¨æ ¡å‡†æ•°æ®é›†ä¸Šè¿›è¡Œé¢„æµ‹ï¼Œç”¨äºæ‹Ÿåˆç­‰æ¸—å›å½’æ ¡å‡†å™¨
    
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        calibration_bearings: æ ¡å‡†æ•°æ®é›†è½´æ‰¿åˆ—è¡¨ï¼ˆé€šå¸¸æ˜¯è®­ç»ƒé›†çš„ç¬¬ä¸€ä¸ªå­é›†ï¼‰
        train_data_dir: è®­ç»ƒæ•°æ®ç›®å½•
        scaler_dir: scalerç›®å½•
        test_bearings: æµ‹è¯•è½´æ‰¿åˆ—è¡¨ï¼ˆç”¨äºç¡®å®šscalerï¼‰
        test_datasets_type: æµ‹è¯•æ•°æ®é›†ç±»å‹
        config: é…ç½®å­—å…¸
        device: è®¾å¤‡
        load_bearing_data_func: åŠ è½½è½´æ‰¿æ•°æ®çš„å‡½æ•°
        test_data_dir: æµ‹è¯•æ•°æ®ç›®å½•ï¼ˆå½“use_test_data_dir=Trueæ—¶ä½¿ç”¨ï¼‰
        use_test_data_dir: æ˜¯å¦ä½¿ç”¨æµ‹è¯•æ•°æ®ç›®å½•åŠ è½½æ ¡å‡†æ•°æ®ï¼ˆé»˜è®¤Falseï¼Œä½¿ç”¨train_data_dirï¼‰
    
    Returns:
        val_target: æ ¡å‡†æ•°æ®é›†çš„çœŸå®å€¼ï¼ˆåå½’ä¸€åŒ–åï¼‰
        val_prediction: æ ¡å‡†æ•°æ®é›†çš„é¢„æµ‹å‡å€¼ï¼ˆåå½’ä¸€åŒ–åï¼‰
        val_pred_std: æ ¡å‡†æ•°æ®é›†çš„é¢„æµ‹æ ‡å‡†å·®
    """
    logger = get_logger()
    
    logger.info("\nğŸ“Š ä½¿ç”¨æ ¡å‡†æ•°æ®é›†è¿›è¡Œé¢„æµ‹ï¼Œç”¨äºæ‹Ÿåˆç­‰æ¸—å›å½’æ ¡å‡†å™¨...")
    logger.info(f"æ ¡å‡†æ•°æ®é›†è½´æ‰¿: {calibration_bearings}")
    
    # æ ¹æ®use_test_data_dirå†³å®šä»å“ªä¸ªç›®å½•åŠ è½½æ•°æ®
    data_dir = test_data_dir if use_test_data_dir and test_data_dir is not None else train_data_dir
    if use_test_data_dir:
        logger.info(f"ä»æµ‹è¯•æ•°æ®ç›®å½•åŠ è½½æ ¡å‡†æ•°æ®: {data_dir}")
    else:
        logger.info(f"ä»è®­ç»ƒæ•°æ®ç›®å½•åŠ è½½æ ¡å‡†æ•°æ®: {data_dir}")
    
    # åŠ è½½æ ¡å‡†æ•°æ®é›†
    calibration_set, calibration_label = load_bearing_data_func(calibration_bearings, data_dir)
    if len(calibration_set) == 0:
        logger.warning("âš ï¸  è­¦å‘Š: æ ¡å‡†æ•°æ®é›†ä¸ºç©ºï¼Œè·³è¿‡æ ¡å‡†")
        return None, None, None
    
    # åˆ›å»ºæ ¡å‡†æ•°æ®åŠ è½½å™¨
    calibration_loader = Data.DataLoader(
        dataset=Data.TensorDataset(calibration_set, calibration_label),
        batch_size=config['test_batch_size'], num_workers=0, drop_last=False,
        pin_memory=True, persistent_workers=False
    )
    
    # åœ¨æ ¡å‡†æ•°æ®é›†ä¸Šè¿›è¡Œé¢„æµ‹
    val_target = []
    val_prediction = []
    val_var_list = []
    val_mu_samples_list = []
    
    model.eval()
    with torch.no_grad():
        for data, label in calibration_loader:
            origin_label = label.tolist()
            val_target += origin_label
            
            data = data.to(device)
            label = label.to(device)
            
            mu_list = []
            log_var = []
            for _ in range(config['forward_pass']):
                mu, sigma, kl = model(data)
                mu_list.append(mu.cpu().numpy())
                log_var.append(sigma.cpu().numpy())
            
            mu_samples = np.stack(mu_list, axis=0)
            sigma_samples = np.stack(log_var, axis=0)
            
            mu_mean = np.mean(mu_samples, axis=0)
            sigma_mean = np.mean(sigma_samples, axis=0)
            val_prediction += mu_mean.squeeze(-1).tolist()
            val_var_list += sigma_mean.squeeze(-1).tolist()
            val_mu_samples_list.append(mu_samples.squeeze(-1))
    
    # åå½’ä¸€åŒ–æ ¡å‡†æ•°æ®é›†é¢„æµ‹ç»“æœ
    # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ ¡å‡†è½´æ‰¿çš„scalerï¼ˆæŒ‰ç…§ notebook ä¸­çš„æ–¹æ³•ï¼‰
    if len(calibration_bearings) > 0:
        first_bearing = calibration_bearings[0]
        bearing_name_for_scaler = first_bearing
        # æ ¹æ®æ•°æ®é›†ç±»å‹ç¡®å®š scaler æ–‡ä»¶å
        if test_datasets_type == 'xjtu_made' or test_datasets_type == 'xjtu_made_v3':
            bearing_name_for_scaler = first_bearing.replace("_labeled", "_labeled_fpt_scaler")
        elif test_datasets_type == 'femto_made':
            bearing_name_for_scaler = first_bearing.replace("_labeled", "_labeled_fpt_scaler")
        
        scaler_path = os.path.join(scaler_dir, test_datasets_type, bearing_name_for_scaler)
        if not os.path.exists(scaler_path):
            # å°è¯•ä¸å¸¦æ•°æ®é›†ç±»å‹å‰ç¼€çš„è·¯å¾„
            scaler_path = os.path.join(scaler_dir, bearing_name_for_scaler)
        
        if os.path.exists(scaler_path):
            scaler = load(scaler_path)
            val_target = scaler.inverse_transform(np.array(val_target).reshape(-1, 1)).reshape(-1)
            val_prediction = scaler.inverse_transform(np.array(val_prediction).reshape(-1, 1)).reshape(-1)
        else:
            logger.warning(f"âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°scaleræ–‡ä»¶ {scaler_path}ï¼Œè·³è¿‡æ ¡å‡†")
            return None, None, None
    else:
        logger.warning("âš ï¸  è­¦å‘Š: æ²¡æœ‰æ ¡å‡†è½´æ‰¿ï¼Œæ— æ³•ç¡®å®šscalerï¼Œè·³è¿‡æ ¡å‡†")
        return None, None, None
    
    val_au = np.array(val_var_list)
    val_eu = np.var(np.concatenate(val_mu_samples_list, axis=1), axis=0) if len(val_mu_samples_list) > 0 else np.zeros(len(val_target))
    
    # è®¡ç®—æ ¡å‡†æ•°æ®é›†çš„é¢„æµ‹æ ‡å‡†å·®ï¼ˆç”¨äºæ ¡å‡†ï¼‰
    val_pred_std = np.sqrt(val_au + val_eu)
    
    logger.info(f"æ ¡å‡†æ•°æ®é›†æ ·æœ¬æ•°: {len(val_target)}")
    logger.info(f"æ ¡å‡†æ•°æ®é›†é¢„æµ‹å‡å€¼èŒƒå›´: [{val_prediction.min():.4f}, {val_prediction.max():.4f}]")
    logger.info(f"æ ¡å‡†æ•°æ®é›†é¢„æµ‹æ ‡å‡†å·®èŒƒå›´: [{val_pred_std.min():.4f}, {val_pred_std.max():.4f}]")
    
    return val_target, val_prediction, val_pred_std


def fit_calibrator(
    val_target: np.ndarray,
    val_prediction: np.ndarray,
    val_pred_std: np.ndarray,
    config: Dict
) -> IsotonicRegressionCalibrator:
    """
    æ‹Ÿåˆç­‰æ¸—å›å½’æ ¡å‡†å™¨
    
    Args:
        val_target: éªŒè¯é›†çœŸå®å€¼
        val_prediction: éªŒè¯é›†é¢„æµ‹å‡å€¼
        val_pred_std: éªŒè¯é›†é¢„æµ‹æ ‡å‡†å·®
        config: é…ç½®å­—å…¸
    
    Returns:
        calibrator: æ‹Ÿåˆå¥½çš„æ ¡å‡†å™¨
    """
    logger = get_logger()
    
    logger.info("\nğŸ”§ æ‹Ÿåˆç­‰æ¸—å›å½’æ ¡å‡†å™¨...")
    
    alpha = 1 - config.get('ci', 0.95)  # ä»é…ç½®ä¸­è·å–ç½®ä¿¡æ°´å¹³
    calibrator = IsotonicRegressionCalibrator(alpha=alpha)
    calibrator.fit(
        y_true=val_target,
        y_pred_mean=val_prediction,
        y_pred_std=val_pred_std
    )
    logger.info("âœ“ æ ¡å‡†å™¨æ‹Ÿåˆå®Œæˆ")
    
    return calibrator


def calibrate_test_results(
    calibrator: IsotonicRegressionCalibrator,
    first_test_results: Dict[str, Dict],
    config: Dict,
    res_dir: str
) -> None:
    """
    å¯¹æµ‹è¯•é›†è¿›è¡Œæ ¡å‡†å¹¶ä¿å­˜ç»“æœ
    
    Args:
        calibrator: æ‹Ÿåˆå¥½çš„æ ¡å‡†å™¨
        first_test_results: ç¬¬ä¸€æ¬¡æµ‹è¯•ç»“æœå­—å…¸
        config: é…ç½®å­—å…¸
        res_dir: ç»“æœä¿å­˜ç›®å½•
    """
    logger = get_logger()
    
    logger.info(f"\n{'='*80}")
    logger.info("ç¬¬äºŒæ¬¡æµ‹è¯•ï¼ˆæ ¡å‡†åï¼‰")
    logger.info(f"{'='*80}")
    
    for bearing_name, first_result in first_test_results.items():
        target = first_result['target']
        prediction = first_result['prediction']
        origin_prediction = first_result['origin_prediction']
        log_var_list = first_result['log_var_list']
        mu_samples = first_result['mu_samples']
        
        # è®¡ç®—æµ‹è¯•é›†çš„é¢„æµ‹æ ‡å‡†å·®
        au = log_var_list
        eu = np.var(origin_prediction, axis=0) if origin_prediction.size > 0 else np.zeros(len(target))
        
        # au æ˜¯å¯¹æ•°æ–¹å·®ï¼ˆlog varianceï¼‰ï¼Œéœ€è¦è½¬æ¢ä¸ºæ–¹å·®ï¼ˆvarianceï¼‰
        # eu å·²ç»æ˜¯æ–¹å·®ï¼ˆvarianceï¼‰
        # æ€»æ–¹å·® = variance_au + euï¼Œç„¶åè®¡ç®—æ ‡å‡†å·®
        variance_au = np.exp(np.clip(au, -20, 10))  # é˜²æ­¢æº¢å‡ºï¼Œclipåˆ°åˆç†èŒƒå›´
        total_variance = variance_au + np.maximum(eu, 0)  # ç¡®ä¿euéè´Ÿ
        test_pred_std = np.sqrt(np.maximum(total_variance, 1e-8))  # é˜²æ­¢è´Ÿå€¼æˆ–é›¶å€¼å¯¼è‡´NaN
        
        # æ£€æŸ¥å¹¶å¤„ç†NaN
        nan_mask = np.isnan(test_pred_std) | np.isinf(test_pred_std)
        if np.any(nan_mask):
            logger.warning(f"âš ï¸  è­¦å‘Š: æ£€æµ‹åˆ° {np.sum(nan_mask)} ä¸ªNaN/Infå€¼ï¼Œä½¿ç”¨é»˜è®¤å€¼æ›¿æ¢")
            default_std = np.nanmedian(test_pred_std[~nan_mask]) if np.any(~nan_mask) else 1.0
            test_pred_std[nan_mask] = default_std if not np.isnan(default_std) else 1.0
        
        # å¯¹æµ‹è¯•é›†è¿›è¡Œæ ¡å‡†
        logger.info(f"\nğŸ“ˆ å¯¹æµ‹è¯•é›† {bearing_name} è¿›è¡Œæ ¡å‡†...")
        y_lower_calibrated, y_upper_calibrated = calibrator.calibrate(
            y_pred_mean=prediction,
            y_pred_std=test_pred_std
        )
        
        # ä»æ ¡å‡†åçš„åŒºé—´åæ¨å‡ºæ ¡å‡†åçš„ä¸ç¡®å®šæ€§
        # ç­‰æ¸—å›å½’æ ¡å‡†å™¨ä½¿ç”¨ z_score * calibrated_uncertainty æ¥è®¡ç®—åŒºé—´
        # æ‰€ä»¥ï¼šcalibrated_uncertainty = (y_upper - y_lower) / (2 * z_score)
        from scipy import stats
        alpha = 1 - config.get('ci', 0.95)
        z_score = stats.norm.ppf(1 - alpha / 2)
        calibrated_uncertainty = (y_upper_calibrated - y_lower_calibrated) / (2 * z_score)
        # è½¬æ¢ä¸ºæ–¹å·®ï¼ˆECEå‡½æ•°éœ€è¦æ–¹å·®ï¼‰
        calibrated_variance = calibrated_uncertainty ** 2
        
        # è¯„ä¼°æ ¡å‡†åçš„æ•ˆæœ
        metrics_after = evaluate_calibration(
            y_true=target,
            y_lower=y_lower_calibrated,
            y_upper=y_upper_calibrated,
            confidence_level=config.get('ci', 0.95)
        )
        
        logger.info(f"\nğŸ“Š æ ¡å‡†åæŒ‡æ ‡ (è½´æ‰¿: {bearing_name}):")
        logger.info(f"  PICP: {metrics_after['PICP']:.6f}")
        logger.info(f"  Coverage Error: {metrics_after['Coverage_Error']:.6f}")
        logger.info(f"  NMPIW: {metrics_after['NMPIW']:.6f}")
        logger.info(f"  MPIW: {metrics_after['MPIW']:.6f}")
        
        # ä¿å­˜æ ¡å‡†åçš„ç»“æœï¼ˆæ–‡ä»¶åæ·»åŠ _calibratedåç¼€ï¼Œä½¿ç”¨os.path.joinç¡®ä¿è·¯å¾„æ­£ç¡®ï¼‰
        calibrated_csv_path = os.path.join(res_dir, f"{bearing_name}_calibrated.csv")
        calibrated_png_path = os.path.join(res_dir, f"{bearing_name}_calibrated.png")
        
        # é‡æ–°è®¡ç®—æŒ‡æ ‡ï¼ˆä½¿ç”¨æ ¡å‡†åçš„åŒºé—´ï¼‰
        y_true = target
        y_pred_mean = prediction
        y_pred_alea = au
        y_pred_epi = eu
        y_pred_std_total = test_pred_std  # ä¿ç•™æ ¡å‡†å‰çš„ä¸ç¡®å®šæ€§ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
        y_pred_std_calibrated = calibrated_uncertainty  # æ ¡å‡†åçš„ä¸ç¡®å®šæ€§
        
        # ä½¿ç”¨æ ¡å‡†åçš„åŒºé—´
        y_lower = y_lower_calibrated
        y_upper = y_upper_calibrated
        
        # è®¡ç®—æŒ‡æ ‡
        R = float(y_true.max() - y_true.min()) if y_true.size > 0 and y_true.max() != y_true.min() else 1.0
        metric_values = {}
        metric_values["MAE"] = float(mae(y_true, y_pred_mean))
        metric_values["RMSE"] = float(rmse(y_true, y_pred_mean))
        metric_values["PICP"] = float(picp(y_true, y_lower, y_upper))
        metric_values["NMPIW"] = float(nmpiw(y_lower, y_upper, R))
        # metric_values["MPIW"] = float(np.mean(y_upper - y_lower))
        # metric_values["Coverage_Error"] = abs(metric_values["PICP"] - config.get('ci', 0.95))
        metric_values["CWC"] = float(cwc(metric_values["PICP"], metric_values["NMPIW"]))
        # ä½¿ç”¨æ ¡å‡†åçš„ä¸ç¡®å®šæ€§è®¡ç®—ECEï¼ˆåæ˜ æ ¡å‡†åçš„ä¸ç¡®å®šæ€§è´¨é‡ï¼‰
        metric_values["ECE"] = float(ece(y_true, y_pred_mean, calibrated_variance))
        metric_values["Sharpness"] = float(sharpness(calibrated_uncertainty, alpha=config.get('ci', 0.95)))
        metric_values["Mean AU"] = float(np.mean(y_pred_alea))
        metric_values["Mean EU"] = float(np.mean(y_pred_epi))
        
        # ä¿å­˜CSVï¼ˆåŒ…å«æŒ‡æ ‡å’Œæ ¡å‡†åçš„é¢„æµ‹åŒºé—´ï¼‰
        # åˆ›å»ºåŒ…å«æ‰€æœ‰æ•°æ®çš„DataFrame
        data_dict = {
            'y_true': y_true,
            'y_pred_mean': y_pred_mean,
            'y_lower_calibrated': y_lower_calibrated,
            'y_upper_calibrated': y_upper_calibrated,
            'y_pred_std_total': y_pred_std_total,  # æ ¡å‡†å‰çš„ä¸ç¡®å®šæ€§
            'y_pred_std_calibrated': y_pred_std_calibrated,  # æ ¡å‡†åçš„ä¸ç¡®å®šæ€§
            'y_pred_alea': y_pred_alea,
            'y_pred_epi': y_pred_epi
        }
        data_df = pd.DataFrame(data_dict)
        
        # ä¿å­˜æ•°æ®åˆ°CSV
        data_df.to_csv(calibrated_csv_path, index=False, encoding='utf-8-sig')
        logger.info(f"âœ“ æ ¡å‡†åæ•°æ®å·²ä¿å­˜åˆ°: {calibrated_csv_path}")
        
        # ä¿å­˜æŒ‡æ ‡åˆ°å•ç‹¬çš„CSVï¼ˆå¯é€‰ï¼Œå¦‚æœéœ€è¦å•ç‹¬ä¿å­˜æŒ‡æ ‡ï¼‰
        metrics_csv_path = os.path.join(res_dir, f"{bearing_name}_calibrated_metrics.csv")
        metrics_df = pd.DataFrame([metric_values])
        metrics_df.to_csv(metrics_csv_path, index=False, encoding='utf-8-sig')
        logger.info(f"âœ“ æ ¡å‡†åæŒ‡æ ‡å·²ä¿å­˜åˆ°: {metrics_csv_path}")
        
        # ç»˜åˆ¶å¹¶ä¿å­˜å›¾ç‰‡
        fig, ax = plt.subplots(figsize=(12, 6))
        x = np.arange(len(y_true))
        ax.plot(x, y_true, 'k-', label="True RUL", linewidth=2)
        ax.plot(x, y_pred_mean, 'b--', label="Predicted Mean", linewidth=1.5)
        ax.fill_between(x, y_lower_calibrated, y_upper_calibrated, 
                      color='lime', alpha=0.25, label='Calibrated Interval')
        ax.legend(loc='best')
        ax.set_title(f"Calibrated - {bearing_name} PI")
        ax.set_xlabel("TIME")
        ax.set_ylabel("RUL")
        plt.tight_layout()
        plt.savefig(calibrated_png_path, dpi=300, bbox_inches='tight')
        plt.close()
        logger.info(f"âœ“ æ ¡å‡†åå›¾ç‰‡å·²ä¿å­˜åˆ°: {calibrated_png_path}")


def run_isotonic_calibration(
    model: torch.nn.Module,
    train_bearings: List[str],
    train_data_dir: str,
    scaler_dir: str,
    test_bearings: List[str],
    test_datasets_type: str,
    first_test_results: Dict[str, Dict],
    config: Dict,
    device: torch.device,
    res_dir: str,
    load_bearing_data_func,
    calibration_bearings: Optional[List[str]] = None,
    calibration_mode: str = 'train_first',
    context_bearings: Optional[List[str]] = None,
    use_same_condition_validation: bool = True,
    test_data_dir: Optional[str] = None
) -> None:
    """
    è¿è¡Œå®Œæ•´çš„ç­‰æ¸—å›å½’æ ¡å‡†æµç¨‹
    
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        train_bearings: è®­ç»ƒé›†è½´æ‰¿åˆ—è¡¨
        train_data_dir: è®­ç»ƒæ•°æ®ç›®å½•
        scaler_dir: scalerç›®å½•
        test_bearings: æµ‹è¯•è½´æ‰¿åˆ—è¡¨
        test_datasets_type: æµ‹è¯•æ•°æ®é›†ç±»å‹
        first_test_results: ç¬¬ä¸€æ¬¡æµ‹è¯•ç»“æœå­—å…¸
        config: é…ç½®å­—å…¸
        device: è®¾å¤‡
        res_dir: ç»“æœä¿å­˜ç›®å½•
        load_bearing_data_func: åŠ è½½è½´æ‰¿æ•°æ®çš„å‡½æ•°
        calibration_bearings: è‡ªå®šä¹‰æ ¡å‡†æ•°æ®é›†è½´æ‰¿åˆ—è¡¨ï¼ˆå¦‚æœæä¾›ï¼Œå°†ä¼˜å…ˆä½¿ç”¨ï¼‰
        calibration_mode: æ ¡å‡†æ•°æ®é€‰æ‹©æ¨¡å¼ï¼Œå¯é€‰å€¼ï¼š
            - 'train_first': ä½¿ç”¨è®­ç»ƒé›†çš„ç¬¬ä¸€ä¸ªå­é›†ï¼ˆé»˜è®¤ï¼‰
            - 'train_all': ä½¿ç”¨æ‰€æœ‰è®­ç»ƒé›†
            - 'context': ä½¿ç”¨ä¸Šä¸‹æ–‡æ•°æ®é›†ï¼ˆéœ€è¦æä¾›context_bearingsï¼‰
            - 'custom': ä½¿ç”¨è‡ªå®šä¹‰è½´æ‰¿åˆ—è¡¨ï¼ˆéœ€è¦æä¾›calibration_bearingsï¼‰
            - 'test': ä½¿ç”¨æµ‹è¯•é›†ä½œä¸ºæ ¡å‡†é›†ï¼ˆä½¿ç”¨test_bearingsï¼‰
        context_bearings: ä¸Šä¸‹æ–‡è½´æ‰¿åˆ—è¡¨ï¼ˆå½“calibration_mode='context'æ—¶ä½¿ç”¨ï¼‰
        use_same_condition_validation: æ˜¯å¦åªä½¿ç”¨ä¸æµ‹è¯•é›†åŒå·¥å†µçš„éªŒè¯é›†ï¼ˆé»˜è®¤Trueï¼‰
        test_data_dir: æµ‹è¯•æ•°æ®ç›®å½•ï¼ˆå½“calibration_mode='test'æ—¶ä½¿ç”¨ï¼‰
    """
    logger = get_logger()
    logger.info(f"âš ï¸  calibration_mode: {calibration_mode}")
    if len(first_test_results) == 0:
        logger.warning("âš ï¸  è­¦å‘Š: æ²¡æœ‰ç¬¬ä¸€æ¬¡æµ‹è¯•ç»“æœï¼Œè·³è¿‡æ ¡å‡†")
        return
    
    logger.info(f"\n{'='*80}")
    logger.info("ç­‰æ¸—å›å½’æ ¡å‡†")
    logger.info(f"{'='*80}")
    # logger.info(f"first_test_results: {first_test_results}")
    # æ ¹æ®å‚æ•°é€‰æ‹©æ ¡å‡†æ•°æ®é›†
    # ä¼˜å…ˆæ£€æŸ¥ calibration_modeï¼Œå¦‚æœæŒ‡å®šäº†ç‰¹å®šæ¨¡å¼ï¼Œåˆ™å¿½ç•¥ calibration_bearings
    if calibration_mode == 'test':
        # ä½¿ç”¨æµ‹è¯•é›†ä½œä¸ºæ ¡å‡†é›†ï¼šæ¯ä¸ªæµ‹è¯•é›†å•ç‹¬è®­ç»ƒä¸€ä¸ªæ ¡å‡†å™¨
        if len(test_bearings) == 0:
            logger.warning("âš ï¸  è­¦å‘Š: æµ‹è¯•é›†ä¸ºç©ºï¼Œè·³è¿‡æ ¡å‡†")
            return
        
        logger.info(f"ä½¿ç”¨æµ‹è¯•é›†ä½œä¸ºæ ¡å‡†é›†ï¼ˆæ¯ä¸ªæµ‹è¯•é›†å•ç‹¬è®­ç»ƒæ ¡å‡†å™¨ï¼‰")
        logger.info(f"æµ‹è¯•é›†è½´æ‰¿: {test_bearings}")
        
        # å¯¹æ¯ä¸ªæµ‹è¯•é›†å•ç‹¬å¤„ç†
        for test_bearing_original in test_bearings:
            # æ‰¾åˆ°å¯¹åº”çš„æµ‹è¯•ç»“æœ
            # é¦–å…ˆå°è¯•ç›´æ¥åŒ¹é…
            matching_key = None
            if test_bearing_original in first_test_results:
                matching_key = test_bearing_original
            else:
                # å°è¯•åŒ¹é…bearing_nameï¼ˆå¯èƒ½åç§°æ ¼å¼ä¸åŒï¼‰
                # æå–åŸºç¡€åç§°ï¼ˆå»æ‰å¯èƒ½çš„åç¼€ï¼‰
                base_name = test_bearing_original
                # å»æ‰å¸¸è§çš„åç¼€
                for suffix in ['_labeled', '_labeled_fpt', '_labeled_fpt_scaler', '_fpt', '_fpt_scaler']:
                    if base_name.endswith(suffix):
                        base_name = base_name[:-len(suffix)]
                        break
                
                # åœ¨first_test_resultsä¸­æŸ¥æ‰¾åŒ¹é…çš„é”®
                for key in first_test_results.keys():
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«åŸºç¡€åç§°
                    if base_name in key or key in base_name:
                        matching_key = key
                        break
                    # ä¹Ÿæ£€æŸ¥å»æ‰åç¼€åçš„key
                    key_base = key
                    for suffix in ['_labeled', '_labeled_fpt', '_labeled_fpt_scaler', '_fpt', '_fpt_scaler']:
                        if key_base.endswith(suffix):
                            key_base = key_base[:-len(suffix)]
                            break
                    if base_name == key_base or base_name in key_base or key_base in base_name:
                        matching_key = key
                        break
            
            if matching_key is None:
                logger.warning(f"âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°æµ‹è¯•é›† {test_bearing_original} çš„æµ‹è¯•ç»“æœï¼Œè·³è¿‡")
                logger.warning(f"  å¯ç”¨çš„æµ‹è¯•ç»“æœé”®: {list(first_test_results.keys())}")
                continue
            
            test_bearing_name = matching_key
            logger.info(f"åŒ¹é…æµ‹è¯•é›†: {test_bearing_original} -> {test_bearing_name}")
            
            first_result = first_test_results[test_bearing_name]
            target = first_result['target']
            prediction = first_result['prediction']
            origin_prediction = first_result['origin_prediction']
            log_var_list = first_result['log_var_list']
            mu_samples = first_result['mu_samples']
            
            logger.info(f"\n{'='*60}")
            logger.info(f"ä¸ºæµ‹è¯•é›† {test_bearing_name} å•ç‹¬è®­ç»ƒæ ¡å‡†å™¨")
            logger.info(f"{'='*60}")
            
            # è®¡ç®—è¯¥æµ‹è¯•é›†çš„é¢„æµ‹æ ‡å‡†å·®
            au = log_var_list
            eu = np.var(origin_prediction, axis=0) if origin_prediction.size > 0 else np.zeros(len(target))
            # eu = np.var(np.concatenate(mu_samples, axis=1), axis=0) if len(mu_samples) > 0 else np.zeros(len(target))
            
            # au æ˜¯å¯¹æ•°æ–¹å·®ï¼ˆlog varianceï¼‰ï¼Œéœ€è¦è½¬æ¢ä¸ºæ–¹å·®ï¼ˆvarianceï¼‰
            # eu å·²ç»æ˜¯æ–¹å·®ï¼ˆvarianceï¼‰
            # æ€»æ–¹å·® = variance_au + euï¼Œç„¶åè®¡ç®—æ ‡å‡†å·®
            variance_au = np.exp(np.clip(au, -20, 10))  # é˜²æ­¢æº¢å‡ºï¼Œclipåˆ°åˆç†èŒƒå›´
            total_variance = variance_au + np.maximum(eu, 0)  # ç¡®ä¿euéè´Ÿ
            test_pred_std = np.sqrt(np.maximum(total_variance, 1e-8))  # é˜²æ­¢è´Ÿå€¼æˆ–é›¶å€¼å¯¼è‡´NaN
            
            # æ£€æŸ¥å¹¶å¤„ç†NaN
            nan_mask = np.isnan(test_pred_std) | np.isinf(test_pred_std)
            if np.any(nan_mask):
                logger.warning(f"âš ï¸  è­¦å‘Š: æ£€æµ‹åˆ° {np.sum(nan_mask)} ä¸ªNaN/Infå€¼ï¼Œä½¿ç”¨é»˜è®¤å€¼æ›¿æ¢")
                default_std = np.nanmedian(test_pred_std[~nan_mask]) if np.any(~nan_mask) else 1.0
                test_pred_std[nan_mask] = default_std if not np.isnan(default_std) else 1.0
            
            # ä½¿ç”¨è¯¥æµ‹è¯•é›†çš„æ•°æ®è®­ç»ƒæ ¡å‡†å™¨
            logger.info(f"ä½¿ç”¨æµ‹è¯•é›† {test_bearing_name} çš„æ•°æ®è®­ç»ƒæ ¡å‡†å™¨...")
            calibrator = fit_calibrator(
                val_target=target,
                val_prediction=prediction,
                val_pred_std=test_pred_std,
                config=config
            )
            
            # ä½¿ç”¨è¯¥æ ¡å‡†å™¨å¯¹è¯¥æµ‹è¯•é›†è¿›è¡Œæ ¡å‡†
            logger.info(f"ä½¿ç”¨è®­ç»ƒå¥½çš„æ ¡å‡†å™¨å¯¹æµ‹è¯•é›† {test_bearing_name} è¿›è¡Œæ ¡å‡†...")
            logger.info(f"prediction.shape: {prediction.shape}")
            logger.info(f"test_pred_std.shape: {test_pred_std.shape}")
            # å¯¹æµ‹è¯•é›†è¿›è¡Œæ ¡å‡†
            y_lower_calibrated, y_upper_calibrated = calibrator.calibrate(
                y_pred_mean=prediction,
                y_pred_std=test_pred_std
            )
            
            # ä»æ ¡å‡†åçš„åŒºé—´åæ¨å‡ºæ ¡å‡†åçš„ä¸ç¡®å®šæ€§
            from scipy import stats
            alpha = 1 - config.get('ci', 0.95)
            # z_score = stats.norm.ppf(1 - alpha / 2)
            calibrated_uncertainty = (y_upper_calibrated - y_lower_calibrated) / (1.96*2)
            calibrated_variance = calibrated_uncertainty ** 2
            
            # è¯„ä¼°æ ¡å‡†åçš„æ•ˆæœ
            metrics_after = evaluate_calibration(
                y_true=target,
                y_lower=y_lower_calibrated,
                y_upper=y_upper_calibrated,
                confidence_level=config.get('ci', 0.95)
            )
            
            logger.info(f"\nğŸ“Š æ ¡å‡†åæŒ‡æ ‡ (è½´æ‰¿: {test_bearing_name}):")
            logger.info(f"  PICP: {metrics_after['PICP']:.6f}")
            logger.info(f"  Coverage Error: {metrics_after['Coverage_Error']:.6f}")
            logger.info(f"  NMPIW: {metrics_after['NMPIW']:.6f}")
            logger.info(f"  MPIW: {metrics_after['MPIW']:.6f}")
            
            # ä¿å­˜æ ¡å‡†åçš„ç»“æœï¼ˆä½¿ç”¨os.path.joinç¡®ä¿è·¯å¾„æ­£ç¡®ï¼‰
            calibrated_csv_path = os.path.join(res_dir, f"{test_bearing_name}_calibrated.csv")
            calibrated_png_path = os.path.join(res_dir, f"{test_bearing_name}_calibrated.png")
            
            # é‡æ–°è®¡ç®—æŒ‡æ ‡ï¼ˆä½¿ç”¨æ ¡å‡†åçš„åŒºé—´ï¼‰
            y_true = target
            y_pred_mean = prediction
            y_pred_alea = au
            y_pred_epi = eu
            y_pred_std_total = test_pred_std
            y_pred_std_calibrated = calibrated_uncertainty
            
            # ä½¿ç”¨æ ¡å‡†åçš„åŒºé—´
            y_lower = y_lower_calibrated
            y_upper = y_upper_calibrated
            
            # è®¡ç®—æŒ‡æ ‡
            R = float(y_true.max() - y_true.min()) if y_true.size > 0 and y_true.max() != y_true.min() else 1.0
            metric_values = {}
            metric_values["MAE"] = float(mae(y_true, y_pred_mean))
            metric_values["RMSE"] = float(rmse(y_true, y_pred_mean))
            metric_values["PICP"] = float(picp(y_true, y_lower, y_upper))
            metric_values["NMPIW"] = float(nmpiw(y_lower, y_upper, R))
            metric_values["CWC"] = float(cwc(metric_values["PICP"], metric_values["NMPIW"], alpha=alpha))
            metric_values["ECE"] = float(ece(y_true, y_pred_mean, calibrated_variance))
            metric_values["Sharpness"] = float(sharpness(calibrated_variance, alpha=alpha))
            metric_values["Aleatoric_Uncertainty"] = float(np.mean(y_pred_alea))
            metric_values["Epistemic_Uncertainty"] = float(np.mean(y_pred_epi))
            
            # ä¿å­˜CSV
            data_dict = {
                'y_true': y_true,
                'y_pred_mean': y_pred_mean,
                'y_lower_calibrated': y_lower_calibrated,
                'y_upper_calibrated': y_upper_calibrated,
                'y_pred_std_total': y_pred_std_total,
                'y_pred_std_calibrated': y_pred_std_calibrated,
                'y_pred_alea': y_pred_alea,
                'y_pred_epi': y_pred_epi
            }
            data_df = pd.DataFrame(data_dict)
            data_df.to_csv(calibrated_csv_path, index=False, encoding='utf-8-sig')
            logger.info(f"âœ“ æ ¡å‡†åæ•°æ®å·²ä¿å­˜åˆ°: {calibrated_csv_path}")
            
            # ä¿å­˜æŒ‡æ ‡
            metrics_csv_path = os.path.join(res_dir, f"{test_bearing_name}_calibrated_metrics.csv")
            metrics_df = pd.DataFrame([metric_values])
            metrics_df.to_csv(metrics_csv_path, index=False, encoding='utf-8-sig')
            logger.info(f"âœ“ æ ¡å‡†åæŒ‡æ ‡å·²ä¿å­˜åˆ°: {metrics_csv_path}")
            
            # ç»˜åˆ¶å¹¶ä¿å­˜å›¾ç‰‡
            fig, ax = plt.subplots(figsize=(12, 6))
            x = np.arange(len(y_true))
            ax.plot(x, y_true, 'k-', label="True RUL", linewidth=2)
            ax.plot(x, y_pred_mean, 'b--', label="Predicted Mean", linewidth=1.5)
            ax.fill_between(x, y_lower_calibrated, y_upper_calibrated, 
                          color='lime', alpha=0.25, label='Calibrated Interval')
            ax.legend(loc='best')
            ax.set_title(f"Calibrated - {test_bearing_name} PI")
            ax.set_xlabel("TIME")
            ax.set_ylabel("RUL")
            plt.tight_layout()
            plt.savefig(calibrated_png_path, dpi=300, bbox_inches='tight')
            plt.close()
            logger.info(f"âœ“ æ ¡å‡†åå›¾ç‰‡å·²ä¿å­˜åˆ°: {calibrated_png_path}")
        
        # å¤„ç†å®Œæ‰€æœ‰æµ‹è¯•é›†åè¿”å›
        return
    elif calibration_mode == 'train_first':
        # ä½¿ç”¨è®­ç»ƒé›†çš„ç¬¬ä¸€ä¸ªå­é›†
        if len(train_bearings) == 0:
            logger.warning("âš ï¸  è­¦å‘Š: è®­ç»ƒé›†ä¸ºç©ºï¼Œè·³è¿‡æ ¡å‡†")
            return
        selected_calibration_bearings = [train_bearings[0]]
        logger.info(f"ä½¿ç”¨è®­ç»ƒé›†çš„ç¬¬ä¸€ä¸ªå­é›†è¿›è¡Œæ ¡å‡†: {selected_calibration_bearings}")
    elif calibration_mode == 'train_all':
        # ä½¿ç”¨æ‰€æœ‰è®­ç»ƒé›†
        if len(train_bearings) == 0:
            logger.warning("âš ï¸  è­¦å‘Š: è®­ç»ƒé›†ä¸ºç©ºï¼Œè·³è¿‡æ ¡å‡†")
            return
        selected_calibration_bearings = train_bearings
        logger.info(f"ä½¿ç”¨æ‰€æœ‰è®­ç»ƒé›†è¿›è¡Œæ ¡å‡†: {selected_calibration_bearings}")
    elif calibration_mode == 'context':
        # ä½¿ç”¨ä¸Šä¸‹æ–‡æ•°æ®é›†
        if context_bearings is None or len(context_bearings) == 0:
            logger.warning("âš ï¸  è­¦å‘Š: ä¸Šä¸‹æ–‡æ•°æ®é›†ä¸ºç©ºï¼Œè·³è¿‡æ ¡å‡†")
            return
        selected_calibration_bearings = context_bearings
        logger.info(f"ä½¿ç”¨ä¸Šä¸‹æ–‡æ•°æ®é›†è¿›è¡Œæ ¡å‡†: {selected_calibration_bearings}")
    elif calibration_mode == 'custom':
        # ä½¿ç”¨è‡ªå®šä¹‰åˆ—è¡¨ï¼ˆéœ€è¦æä¾›calibration_bearingsï¼‰
        if calibration_bearings is None or len(calibration_bearings) == 0:
            logger.warning("âš ï¸  è­¦å‘Š: è‡ªå®šä¹‰æ ¡å‡†æ•°æ®é›†ä¸ºç©ºï¼Œè·³è¿‡æ ¡å‡†")
            return
        # å¦‚æœå¯ç”¨åŒå·¥å†µç­›é€‰ï¼Œåˆ™åªä½¿ç”¨ä¸æµ‹è¯•é›†åŒå·¥å†µçš„éªŒè¯é›†
        if use_same_condition_validation:
            filtered_calibration = filter_validation_by_condition(calibration_bearings, test_bearings)
            if len(filtered_calibration) > 0:
                selected_calibration_bearings = filtered_calibration
                logger.info(f"ä½¿ç”¨ä¸æµ‹è¯•é›†åŒå·¥å†µçš„éªŒè¯é›†è¿›è¡Œæ ¡å‡†: {selected_calibration_bearings}")
                logger.info(f"æµ‹è¯•é›†å·¥å†µ: {[condition_group(b) for b in test_bearings]}")
            else:
                logger.warning(f"âš ï¸  è­¦å‘Š: éªŒè¯é›†ä¸­æ²¡æœ‰ä¸æµ‹è¯•é›†åŒå·¥å†µçš„è½´æ‰¿ï¼Œä½¿ç”¨å…¨éƒ¨éªŒè¯é›†: {calibration_bearings}")
                selected_calibration_bearings = calibration_bearings
        else:
            selected_calibration_bearings = calibration_bearings
            logger.info(f"ä½¿ç”¨è‡ªå®šä¹‰æ ¡å‡†æ•°æ®é›†: {selected_calibration_bearings}")
    elif calibration_bearings is not None and len(calibration_bearings) > 0:
        # å¦‚æœæä¾›äº†calibration_bearingsä½†æ²¡æœ‰æŒ‡å®šæ¨¡å¼ï¼Œåˆ™ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å¼
        # å¦‚æœå¯ç”¨åŒå·¥å†µç­›é€‰ï¼Œåˆ™åªä½¿ç”¨ä¸æµ‹è¯•é›†åŒå·¥å†µçš„éªŒè¯é›†
        if use_same_condition_validation:
            filtered_calibration = filter_validation_by_condition(calibration_bearings, test_bearings)
            if len(filtered_calibration) > 0:
                selected_calibration_bearings = filtered_calibration
                logger.info(f"ä½¿ç”¨ä¸æµ‹è¯•é›†åŒå·¥å†µçš„éªŒè¯é›†è¿›è¡Œæ ¡å‡†: {selected_calibration_bearings}")
                logger.info(f"æµ‹è¯•é›†å·¥å†µ: {[condition_group(b) for b in test_bearings]}")
            else:
                logger.warning(f"âš ï¸  è­¦å‘Š: éªŒè¯é›†ä¸­æ²¡æœ‰ä¸æµ‹è¯•é›†åŒå·¥å†µçš„è½´æ‰¿ï¼Œä½¿ç”¨å…¨éƒ¨éªŒè¯é›†: {calibration_bearings}")
                selected_calibration_bearings = calibration_bearings
        else:
            selected_calibration_bearings = calibration_bearings
            logger.info(f"ä½¿ç”¨è‡ªå®šä¹‰æ ¡å‡†æ•°æ®é›†: {selected_calibration_bearings}")
    else:
        logger.warning(f"âš ï¸  è­¦å‘Š: æœªçŸ¥çš„æ ¡å‡†æ¨¡å¼ '{calibration_mode}'ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å¼ 'train_first'")
        if len(train_bearings) == 0:
            logger.warning("âš ï¸  è­¦å‘Š: è®­ç»ƒé›†ä¸ºç©ºï¼Œè·³è¿‡æ ¡å‡†")
            return
        selected_calibration_bearings = [train_bearings[0]]
        logger.info(f"ä½¿ç”¨è®­ç»ƒé›†çš„ç¬¬ä¸€ä¸ªå­é›†è¿›è¡Œæ ¡å‡†: {selected_calibration_bearings}")
    
    print("selected_calibration_bearings", selected_calibration_bearings)
    # 1. åœ¨æ ¡å‡†æ•°æ®é›†ä¸Šè¿›è¡Œé¢„æµ‹
    # å¦‚æœä½¿ç”¨æµ‹è¯•é›†ä½œä¸ºæ ¡å‡†é›†ï¼Œéœ€è¦ä»æµ‹è¯•æ•°æ®ç›®å½•åŠ è½½æ•°æ®
    use_test_data_dir = (calibration_mode == 'test')
    val_target, val_prediction, val_pred_std = predict_on_calibration_dataset(
        model=model,
        calibration_bearings=selected_calibration_bearings,
        train_data_dir=train_data_dir,
        scaler_dir=scaler_dir,
        test_bearings=test_bearings,
        test_datasets_type=test_datasets_type,
        config=config,
        device=device,
        load_bearing_data_func=load_bearing_data_func,
        test_data_dir=test_data_dir,
        use_test_data_dir=use_test_data_dir
    )
    
    if val_target is None or val_prediction is None or val_pred_std is None:
        logger.warning("âš ï¸  è­¦å‘Š: æ ¡å‡†æ•°æ®é›†é¢„æµ‹å¤±è´¥ï¼Œè·³è¿‡æ ¡å‡†")
        return
    
    # 2. æ‹Ÿåˆç­‰æ¸—å›å½’æ ¡å‡†å™¨
    calibrator = fit_calibrator(
        val_target=val_target,
        val_prediction=val_prediction,
        val_pred_std=val_pred_std,
        config=config
    )
    
    # 3. å¯¹æµ‹è¯•é›†è¿›è¡Œæ ¡å‡†å¹¶ä¿å­˜ç»“æœ
    calibrate_test_results(
        calibrator=calibrator,
        first_test_results=first_test_results,
        config=config,
        res_dir=res_dir
    )

