{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import argparse\n",
    "\n",
    "# 命令行参数解析\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--train_xj', type=str, default=None, help='训练轴承列表, 逗号分隔')\n",
    "parser.add_argument('--test_xj', type=str, default=None, help='测试轴承列表, 逗号分隔')\n",
    "parser.add_argument('--context_xj', type=str, default=None, help='上下文轴承列表, 逗号分隔')\n",
    "parser.add_argument('--validation_xj', type=str, default=None, help='验证轴承列表, 逗号分隔')\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "with open('../../config/cnnlstm_config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# 解析命令行参数，优先级高于config\n",
    "if args.train_xj is not None:\n",
    "    TRAIN_xj = [x.strip() for x in args.train_xj.split(',') if x.strip()]\n",
    "else:\n",
    "    TRAIN_xj = config['train_bearings']\n",
    "if args.test_xj is not None:\n",
    "    TEST_xj = [x.strip() for x in args.test_xj.split(',') if x.strip()]\n",
    "else:\n",
    "    TEST_xj = config['test_bearings']\n",
    "if args.context_xj is not None:\n",
    "    CONTEXT_xj = [x.strip() for x in args.context_xj.split(',') if x.strip()]\n",
    "else:\n",
    "    CONTEXT_xj = config['context_bearings']\n",
    "if args.validation_xj is not None:\n",
    "    VALIDATION_xj = [x.strip() for x in args.validation_xj.split(',') if x.strip()]\n",
    "else:\n",
    "    VALIDATION_xj = config['validation_bearings']\n",
    "\n",
    "batch_size = config['batch_size']\n",
    "test_batch_size = config['test_batch_size']\n",
    "window_size = config['window_size']\n",
    "input_dim = config['input_dim']\n",
    "output_dim = config['output_dim']\n",
    "conv_archs = config['conv_archs']\n",
    "conv_archs = [(1, arch) for arch in conv_archs]  # 将[32, 64]转换为[(1, 32), (1, 64)]\n",
    "hidden_layer_sizes = config['hidden_layer_sizes']\n",
    "epochs = config['epochs']\n",
    "learn_rate = config['learn_rate']\n",
    "seed = config['seed']\n",
    "patience = config['patience']\n",
    "opt = config['opt']\n",
    "\n",
    "if patience == \"inf\":\n",
    "    patience = epochs\n",
    "else:\n",
    "    patience = int(patience)\n",
    "\n",
    "# 加载数据\n",
    "import torch\n",
    "from joblib import dump, load\n",
    "import torch.utils.data as Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import os\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "# 参数与配置\n",
    "torch.manual_seed(seed)  # 设置随机种子，以使实验结果具有可重复性\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"当前运行设备: {device}\")\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "FPT_dict_xj = {'Bearing1_1': 76, 'Bearing1_2': 44, 'Bearing1_3': 60, 'Bearing1_4': 0, 'Bearing1_5': 39,\n",
    "            'Bearing2_1': 455, 'Bearing2_2': 48, 'Bearing2_3': 327, 'Bearing2_4': 32, 'Bearing2_5': 141,\n",
    "            'Bearing3_1': 2344, 'Bearing3_2': 0, 'Bearing3_3': 340, 'Bearing3_4': 1418, 'Bearing3_5': 9}\n",
    "# 加载数据集\n",
    "def dataloader(batch_size, test_batch_size, workers=os.cpu_count(), window_size=7):\n",
    "    # 根据TRAIN_xj，TEST_xj从datasetresult/xjtu_made目录下加载相应训练集和测试集的数据和标签\n",
    "    def get_data_and_labels(bearing_list, data_dir='../../datasetresult/xjtu_made', window_size=window_size, is_test=False):\n",
    "        data_list = []\n",
    "        label_list = []\n",
    "        for bearing in bearing_list:\n",
    "            # 文件名格式: c*_Bearing*_*_data, c*_Bearing*_*_label\n",
    "            # 只要包含该bearing名即可\n",
    "            data_files = [f for f in os.listdir(data_dir) if bearing in f and f.endswith('_data')]\n",
    "            label_files = [f for f in os.listdir(data_dir) if bearing in f and f.endswith('_label')]\n",
    "            # 按文件名排序，确保一一对应\n",
    "            data_files.sort()\n",
    "            label_files.sort()\n",
    "            for data_file, label_file in zip(data_files, label_files):\n",
    "                data = load(os.path.join(data_dir, data_file))\n",
    "                label = load(os.path.join(data_dir, label_file))\n",
    "                # 如果是测试集，且FPT_dict_xj中有该bearing，截取FPT值往后的数据\n",
    "                if is_test and bearing in FPT_dict_xj:\n",
    "                    fpt = FPT_dict_xj[bearing]\n",
    "                    # FPT为0时，保留全部数据\n",
    "                    if fpt > 0:\n",
    "                        data = data[fpt:]\n",
    "                        label = label[fpt:]\n",
    "                data_list.append(data)\n",
    "                label_list.append(label)\n",
    "        # 拼接所有轴承的数据\n",
    "        if len(data_list) > 0:\n",
    "            data_all = torch.cat([torch.tensor(d) if not isinstance(d, torch.Tensor) else d for d in data_list], dim=0)\n",
    "            label_all = torch.cat([torch.tensor(l) if not isinstance(l, torch.Tensor) else l for l in label_list], dim=0)\n",
    "        else:\n",
    "            data_all = torch.empty(0)\n",
    "            label_all = torch.empty(0)\n",
    "        return data_all, label_all\n",
    "\n",
    "    # 加载训练集和测试集\n",
    "    train_set, train_label = get_data_and_labels(TRAIN_xj)\n",
    "    context_set, context_label = get_data_and_labels(CONTEXT_xj)\n",
    "    test_set, test_label = get_data_and_labels(TEST_xj, is_test=True)\n",
    "    validation_set, validation_label = get_data_and_labels(VALIDATION_xj)\n",
    "    print(train_set.shape, train_label.shape)\n",
    "    print(test_set.shape, test_label.shape)\n",
    "    \n",
    "    # 加载数据\n",
    "    train_loader = Data.DataLoader(dataset=Data.TensorDataset(train_set, train_label),\n",
    "                                   batch_size=batch_size, num_workers=workers, drop_last=False)\n",
    "    context_loader = Data.DataLoader(dataset=Data.TensorDataset(context_set, context_label),\n",
    "                                   batch_size=batch_size, num_workers=workers, drop_last=False)\n",
    "    test_loader = Data.DataLoader(dataset=Data.TensorDataset(test_set, test_label),\n",
    "                                  batch_size=test_batch_size, num_workers=workers, drop_last=True)\n",
    "    validation_loader = Data.DataLoader(dataset=Data.TensorDataset(validation_set, validation_label),\n",
    "                                   batch_size=test_batch_size, num_workers=workers, drop_last=True)\n",
    "    return train_loader, context_loader, test_loader, validation_loader\n",
    "\n",
    "# 加载数据\n",
    "train_loader, context_loader, test_loader, validation_loader = dataloader(batch_size, test_batch_size, window_size=window_size)\n",
    "\n",
    "print(len(train_loader.dataset))\n",
    "print(len(context_loader.dataset))\n",
    "print(len(test_loader.dataset))\n",
    "print(len(validation_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# 定义 CNNLSTMModel 模型\n",
    "class CNNLSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, conv_archs, hidden_layer_sizes, output_dim):\n",
    "        \"\"\"\n",
    "        params:\n",
    "        input_dim          : 输入数据的维度\n",
    "        conv_archs         : cnn 网络结构\n",
    "        hidden_layer_sizes : lstm 隐层的数目和维度\n",
    "        output_dim         : 输出维度数\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # 参数\n",
    "        # CNN参数\n",
    "        self.conv_arch = conv_archs # cnn网络结构\n",
    "        self.input_channels = input_dim # 输入通道数\n",
    "        self.cnn_features = self.make_layers()\n",
    "\n",
    "        # LSTM参数\n",
    "        self.num_layers = len(hidden_layer_sizes)  # lstm层数\n",
    "        self.lstm_layers = nn.ModuleList()  # 用于保存LSTM层的列表\n",
    "        # 定义第一层LSTM   \n",
    "        self.lstm_layers.append(nn.LSTM(conv_archs[-1][-1], hidden_layer_sizes[0], batch_first=True))\n",
    "        # 定义后续的LSTM层\n",
    "        for i in range(1, self.num_layers):\n",
    "                self.lstm_layers.append(nn.LSTM(hidden_layer_sizes[i-1], hidden_layer_sizes[i], batch_first=True))\n",
    "\n",
    "        # 定义线性层\n",
    "        self.mu  = nn.Linear(hidden_layer_sizes[-1], output_dim)\n",
    "        self.sigma = nn.Linear(hidden_layer_sizes[-1], output_dim)\n",
    "\n",
    "    # CNN卷积池化结构\n",
    "    def make_layers(self):\n",
    "        layers = []\n",
    "        for (num_convs, out_channels) in self.conv_arch:\n",
    "            for _ in range(num_convs):\n",
    "                layers.append(nn.Conv1d(self.input_channels, out_channels, kernel_size=3, padding=1))\n",
    "                layers.append(nn.ReLU(inplace=True))\n",
    "                self.input_channels = out_channels\n",
    "            # layers.append(nn.MaxPool1d(kernel_size=2, stride=2))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input_seq): \n",
    "        # CNN 卷积池化\n",
    "        # CNN 网络输入[batch, dim, seq_length]\n",
    "        #改变输入形状\n",
    "        input_seq = input_seq.permute(0, 2, 1)\n",
    "        cnn_features = self.cnn_features(input_seq)\n",
    "        # print(cnn_features.size()) # torch.Size([32, 64, 1])\n",
    "\n",
    "        # 送入 LSTM 层\n",
    "        #改变输入形状，lstm 适应网络输入[batch, seq_length, dim]\n",
    "        lstm_out = cnn_features.permute(0, 2, 1)\n",
    "        for lstm in self.lstm_layers:\n",
    "            lstm_out, _= lstm(lstm_out)  ## 进行一次LSTM层的前向传播  # torch.Size([32, 1, 128])\n",
    "        mu = self.mu(lstm_out)\n",
    "        sigma = self.sigma(lstm_out)\n",
    "        return mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from loss_function import compute_au_nll, compute_au_nll_with_pos\n",
    "\n",
    "model = CNNLSTMModel(input_dim, conv_archs, hidden_layer_sizes, output_dim)  \n",
    "\n",
    "loss_function = compute_au_nll_with_pos  # loss\n",
    "\n",
    "if config['opt'] == 'Adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(), learn_rate)  # 优化器\n",
    "elif config['opt'] == 'AdamW':\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), learn_rate)  # 使用AdamW优化器\n",
    "elif config['opt'] == 'SGD':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), learn_rate)  # 使用SGD优化器\n",
    "else:\n",
    "    raise ValueError(f\"Invalid optimizer: {config['opt']}\")\n",
    "\n",
    "# 看下这个网络结构总共有多少个参数\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rc(\"font\", family='Microsoft YaHei')\n",
    "\n",
    "def model_train(epochs, model, optimizer, loss_function, train_loader, device):\n",
    "    model = model.to(device)\n",
    "\n",
    "    # 最低MSE  \n",
    "    minimum_mse = 1000.\n",
    "    # 最佳模型\n",
    "    best_model = model\n",
    "\n",
    "    train_mse = []     # 记录在训练集上每个epoch的 MSE 指标的变化情况   平均值\n",
    "  \n",
    "    # 计算模型运行时间\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "         # 训练\n",
    "        model.train()\n",
    "        train_mse_loss = []    #保存当前epoch的MSE loss和\n",
    "        for seq, labels in train_loader: \n",
    "            seq, labels = seq.to(device), labels.to(device)\n",
    "            # 每次更新参数前都梯度归零和初始化\n",
    "            optimizer.zero_grad()\n",
    "            # 前向传播\n",
    "            mu, sigma = model(seq)  #   torch.Size([16, 10])\n",
    "            # 损失计算\n",
    "            loss = loss_function(labels, mu, sigma)\n",
    "            train_mse_loss.append(loss.item()) # 计算 MSE 损失\n",
    "            # 反向传播和参数更新\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        #     break\n",
    "        # break\n",
    "        # 计算总损失\n",
    "        train_av_mseloss = np.average(train_mse_loss) # 平均\n",
    "        train_mse.append(train_av_mseloss)\n",
    "        print(f'Epoch: {epoch+1:2} train_MSE-Loss: {train_av_mseloss:10.8f}')\n",
    "       \n",
    "        # 如果当前模型的 MSE 低于于之前的最佳准确率，则更新最佳模型\n",
    "        #保存当前最优模型参数\n",
    "        if train_av_mseloss < minimum_mse:\n",
    "            minimum_mse = train_av_mseloss\n",
    "            best_model = model# 更新最佳模型的参数\n",
    "         \n",
    "    # 保存最后的参数\n",
    "    # torch.save(model, 'final_model_cnn_lstm.pt')\n",
    "    # 保存最好的参数\n",
    "    torch.save(best_model, 'best_model_cnnlstm.pt')\n",
    "    print(f'\\nDuration: {time.time() - start_time:.0f} seconds')\n",
    "    \n",
    "    # 可视化\n",
    "    plt.plot(range(epochs), train_mse, color = 'b',label = 'train_MSE-loss')\n",
    "    plt.legend()\n",
    "    plt.show()   #显示 lable \n",
    "    print(f'min_MSE: {minimum_mse}')\n",
    "\n",
    "#  模型训练\n",
    "model_train(epochs, model, optimizer, loss_function, train_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试集评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型 \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载模型的状态字典\n",
    "model = torch.load('best_model_cnnlstm.pt', weights_only=False)\n",
    "model = model.to(device)\n",
    "\n",
    "# 预测数据\n",
    "test_origin_data = []\n",
    "test_pre_data = []\n",
    "with torch.no_grad():\n",
    "    for data, label in test_loader:\n",
    "        # 原始标签\n",
    "        origin_lable = label.tolist()\n",
    "        test_origin_data += origin_lable\n",
    "        model.eval()  # 将模型设置为评估模式\n",
    "        \n",
    "        # 预测\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        mu, sigma = model(data)  # 对测试集进行预测\n",
    "        pred = mu.tolist()\n",
    "        sigma = sigma.tolist()\n",
    "        test_pre_data += pred        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "# 反归一化处理\n",
    "# 使用相同的均值和标准差对预测结果进行反归一化处理\n",
    "# 反标准化\n",
    "scaler_dir = '../../datasetresult/xjtu_made'\n",
    "scaler_filename = f'{TEST_xj[0]}_labeled_scaler'\n",
    "all_files = os.listdir(scaler_dir)\n",
    "matched_files = [f for f in all_files if f.endswith(scaler_filename)]\n",
    "if not matched_files:\n",
    "    raise FileNotFoundError(f\"未找到匹配 {TEST_xj[0]} 的 scaler 文件, 目录: {scaler_dir}\")\n",
    "scaler_path = os.path.join(scaler_dir, matched_files[0])\n",
    "scaler = load(scaler_path)\n",
    "\n",
    "test_origin_data = scaler.inverse_transform(np.array(test_origin_data).reshape(-1, 1)).reshape(-1)\n",
    "test_pre_data = scaler.inverse_transform(np.array(test_pre_data).reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "\n",
    "# 模型分数\n",
    "score = r2_score(test_origin_data, test_pre_data)\n",
    "print('测试集上 模型分数-R^2:',score)\n",
    "\n",
    "print('*'*50)\n",
    "# 训练集上的预测误差\n",
    "test_mse = mean_squared_error(test_origin_data, test_pre_data)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(test_origin_data, test_pre_data)\n",
    "print('测试数据集上的均方误差-MSE: ',test_mse)\n",
    "print('测试数据集上的均方根误差-RMSE: ',test_rmse)\n",
    "print('测试数据集上的平均绝对误差-MAE: ',test_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rc(\"font\", family='Microsoft YaHei')\n",
    "\n",
    "# 可视化结果\n",
    "plt.figure(figsize=(12, 6), dpi=100)\n",
    "plt.plot(test_origin_data, label='真实寿命',color='orange')  # 真实值\n",
    "plt.plot(test_pre_data, label='CNN-LSTM 预测值',color='green')  # 预测值\n",
    "\n",
    "# plt.plot([-1,170],[2.0*0.7,2.0*0.7],c='black',lw=1,ls='--')  # 临界点直线  可自己调整位置\n",
    "\n",
    "plt.xlabel('运行周期/10s', fontsize=12)\n",
    "plt.ylabel('寿命百分比', fontsize=12)\n",
    "plt.title(f'{TEST_xj[0]} 预测结果', fontsize=16)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 保存数据\n",
    "dump(test_origin_data, '../画图对比/cnn_lstm_origin') \n",
    "dump(test_pre_data, '../画图对比/cnn_lstm_pre') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
